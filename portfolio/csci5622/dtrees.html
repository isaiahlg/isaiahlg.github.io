<!DOCTYPE html>
<html>

<head>
    <title>Isaiah LG - ML/D-Trees</title>
    <link rel="stylesheet" href="/style.css">
    <link rel="icon" href="/assets/baobab.png">
    <script src="https://kit.fontawesome.com/f4ea09cda5.js" crossorigin="anonymous"></script>
    <!-- for rendering equations from TeX to HTML -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
    <button class="back-button" onclick="window.location.href='/csci5622/home.html';"><i class="fa-solid fa-arrow-left"></i></button>
    <div class="content">
        <div class="textbox">
            <h1><i class="fa fa-tree"></i> Decision Trees</h1>
            <h2>Overview</h2>
            Decision Trees are a popular machine learning algorithm used to solve a variety of problems, such as classification and regression. At a high level, Decision Trees work by splitting the data based on different features or attributes, in order to create a tree-like structure of decision rules. Decision trees are one of the few non-linear ways of partitioning data, making them especially powerful for mixed data that includes both qualitative and quantitative variables. 
            <figure>
                <img class="figure" src="/portfolio/csci5622/figures/dtrees/structure.png">
                <figcaption>
                    This diagram shows the anatomy of a decision tree. After each node, the data is split based on a given variable designed to reduce the entropy as much as possible. The splitting continues until one ends up at a terminal node. (Chauhan, 2022)
                </figcaption>
            </figure>
            To create a Decision Tree, the Decision Tree algorithm will look at the data and try to find the most important features that are highly correlated with the target variable  It will then split the data based on these features, creating a tree-like structure where each branch represents a decision rule. In order to determine which feature to split on and how to determine the threshold for the split, the Decision Tree algorithm evaluates "goodness" of a split by calculating the "Information Gain" for the given split. Information Gain is defined as the reduction in either the Gini or Entropy of the data. 
            

            <h4>Example</h4>
            Below, we will work through an example of a simple decision tree with two categories and two nodes. We'll calculate the entropy of the root node and the children. The following example was inspired by one written by Stathis Kamperis available on his <a href="https://ekamperi.github.io/machine%20learning/2021/04/13/gini-index-vs-entropy-decision-trees.html" target="_blank">GitHub</a>
            <figure>
                <img class="figure" src="/portfolio/csci5622/figures/dtrees/example.png">
                <figcaption>This figure shows an example of decision tree trying to categorize the stars and circles based on an attribute "balance". Image is from from “Provost, Foster; Fawcett, Tom. Data Science for Business: What You Need to Know about Data Mining and Data-Analytic Thinking”.</figcaption>
            </figure>
            <h4>Gini</h4>
            Gini measures the impurity of a node in the decision tree. The lower the GINI score, the more pure the node is (i.e. the more homogeneous the data is within that node). 
            The equation for Gini is:
               
                \[Gini = 1 - \sum_ {i=1}^C (p_i)^2 \]
            
            where \(i\) represents each possible value of the target variable on the given node of the decision tree with \(C\) possible values. \(p_i\) is the probability of each of those values. Here, we are squaring the probability of each of the values and then subtracting them from 1. The more imbalanced the data, the lower the value of the Gini will be. \(Gini \in [0,0.5]\). The minimum value of 0.0 occurs when the probability of one element is 1.0 (100%) and thus 0.0 for all others. The maximum value of 0.5 occurs when there are two target values (\(C=2\)) and there is an equal probability of both (\(p_i = 0.5\)). When the target values are evenly distributed, then the value of \(Entropy = 1 - {1 \over C}\). Optimizing for Gini tends to lead to fewer, larger nodes in the tree. Let's start by caluclating the Gini of the root node:
            \[G(Root) = 1 - \sum_{i=1}^2 p_i^2 = 1 - p_1^2 - p_2^2 = 1 - ({14\over30})^2 - ({16\over30})^2 \approx 0.50 \]

           This is essentially the maximum Gini of 0.50, meaning the data are almost perfectly balanced. We can calculate the Gini impurity of the left leaf node:

            \[G(Bal < 50i) = 1−\sum_{i=1}^2 p_i^2=1−p_1^2−p_2^2 = 1−({12\over13})^2−({1\over13})^2 \approx 0.14\]

            ...and the Gini impurity of the right leaf node:
            
            \[G(Bal ≥ 50i) = 1−\sum_{i=1}^2 p_i^2=1−p_1^2−p_2^2=1−({4\over17})^2−({13\over17})^2 \approx 0.36\]

            We notice that the left node has a lower Gini impurity index, which we’d expect since G measures impurity, and the left node is purer relative to the right one. To calculate the information gain, we need to compare the Gini before the split with the weighted sum of the Gini after the split. 

            \[InformationGain = G(Before) - G(After) = 0.50 - {(13*0.14) + (17*0.36) \over 30 } \approx 0.24 \]
            
            <h4>Entropy</h4>
            Entropy, on the other hand, is a measure of the uncertainty or randomness within a node. A lower entropy score indicates a more certain or predictable outcome. The equation for Entropy is:
                \[Entropy = \sum_{i=1}^C -p_i * log_2(p_i) \]

            To calculate entropy of a node, we are multiplying the negative probability of each value times the logarithm base 2 of each probability. Since the probability is always bewteen \([0,1]\), the \(log_2(p_i)\) will always be negative, and thus is flipped back to positive when multiplied by \(-p_i\). That's how \(Entropy \in [0,log_2(k)]\). Optimizing for Entropy tends to favor splitting into more smaller nodes in the tree. Now let’s repeat these calculations from above for the entropy. When we use \(log_2\) in the entropy formula, we get the result in bits. 
            
            Let's begin with the root node:
            
            \[E(Root) = \sum_{i=1}^2 -p_i*log_2(p_i) = −p1*log_{2}(p_1)−p2*log_{2}(p_2)\]
            \[= -{14\over30}*log_2({14\over30}) - {16\over30}*log_2({16\over30})) = -0.4667*-1.0995 - 0.5333*-0.90698 \approx 0.996 bits\]
            
            This is very close to 1, the maximum entropy for two options, meaning the data are extremely disordered. Next, let's proceed with the Entropy of the left leaf node:
            
            \[E(Bal < 50i)=\sum_{i=1}^2 −p_{i}*log_{2}(p_i) = −p1*log_{2}(p_1)−p2*log_{2}(p_2)\]
            \[=−{12\over13}*log_{2}g({12\over13})−{1\over13}*log_{2}({1\over13}) \approx 0.36 bits\]
            
            Let’s calculate the entropy of the right leaf node as well:
            
            \[E(Bal ≥ 50i)=\sum_{i=1}^2 −p_{i}*log_{2}(p_i) = −p1*log_{2}(p_1)−p2*log_{2}(p_2)\]
            \[=−{4\over17}*log_{2}({4\over17})−{13\over17}*log_{2}({13\over17}) \approx 0.79 bits\]

            Finally, we can calculate the information gain in terms of Entropy:

            \[InformationGain = E(Before) - E(After) = 0.50 - {(13*0.14) + (17*0.36) \over 30 } \approx 0.24 bits \]

            By using these methods to measure the "goodness" of a split, the Decision Tree algorithm is able to find the optimal split points that will result in the most accurate predictions. If all of the variables used for the decision tree, there will be many but finite possible trees. However, if even a single numerical variable is introduced, there becomes an infinite number of ways that the nodes can be split, and thus an infinite number of possible trees. Not all trees are equally effective in making accurate predictions, and so it's important to use methods like GINI and Entropy to guide the creation of the most effective decision tree.
            
            <h2>Data Prep</h2>
            <p>
            In order to do supervised learning, one needs a dataset with a label. For this dataset, the target variable of interest in the wealth index quantile. There are 5 groups, and it is interesting to see which variables are the strongest predictors of this. The decision tree is analyzing the relavance of each other variable in the survey towards predicting the wealth quantile. The variables of interest are the assets used during Association Rule Mining, but also the demographic data used in clustering. Since this will be mixed data, it will need to be performed in R. 
            </p><p>
            A copy of the full dataset can be found <a href="https://github.com/isaiahlg/csci5622mod3/blob/main/proj/data/sl19.rds" target="_blank">here</a>. To clean the data, the full survey dataset is pulled again from DHS and then trimmed down to just the variables of interest. Next, some additional cleaning is performed to remove missing or unknown values. Next, categorical variables (including the label) are converted to factors to ensure like values are grouped by R. A copy of the clean dataset can be found <a href="https://github.com/isaiahlg/csci5622mod3/blob/main/proj/data/sl19dtreeClean.rds" target="_blank">here</a>. Before going further, we need to check that the data are relatively balanced between the five wealth index groups. 
            <figure>
                <img class="figure" src="/portfolio/csci5622/figures/dtrees/balanceAll.png">
                <figcaption>A histogram of the wealth index groups for the entire dataset. Here we can see that the data are relatively balanced between the groups, with the largest group < 20% larger than the smallest group.</figcaption>
            </figure>
            Finally, the data need to be split into a training set and testing set. It is important that the test data and the training data are kept disjoint, meaning that the model is never trained on the test data. This maximizes the performance of the model on never before seen data because it ensures that the model isn't just simply fitted to all of the data at hand. Because the data contain over 13,000 rows, there is not much concern of having sufficiently large samples. To split the data, 80% of the rows are randomly sampled without replacement into a training data set, with the remaining 20% serving as the testing data set. The two data sets remain disjoint for proper evaluation of the decision tree. Below are snapshots of the training and testing dataset accordingly. Click on the images to get to RDS files on GitHub.
            <figure>
                <a href="https://github.com/isaiahlg/csci5622mod3/blob/main/proj/data/testdf.rds" target="_blank">
                    <img class="figure" src="/portfolio/csci5622/figures/dtrees/traindf.png">
                </a>
                <figcaption>A snapshot of the training data</figcaption>
            </figure>
            <figure>
                <a href="https://github.com/isaiahlg/csci5622mod3/blob/main/proj/data/traindf.rds" target="_blank"></a>
                    <img class="figure" src="/portfolio/csci5622/figures/dtrees/testdf.png">
                </a>
                <figcaption>A snapshot of the test data</figcaption>
            </figure>  
            Finally, we need to check to see that the test and the train data are still balanced among the categories with histograms:
            <figure>
                <img class="figure2" src="/portfolio/csci5622/figures/dtrees/balanceTest.png">
                <img class="figure2" src="/portfolio/csci5622/figures/dtrees/balanceTrain.png">
            </figure>
            </p>
            <h2>Code</h2>
            Find all of the code use to clean the data and run the decision trees in R on GitHub <a href="https://github.com/isaiahlg/csci5622mod3/blob/main/proj/dtrees.Rmd" target="_blank">here.</a> 
            <figure>
                <img class="figure" src="/portfolio/csci5622/figures/dtrees/codeSnippet.png">
            </figure>

            <h2>Results</h2>
            The resulting decision tree is pictured below:
            <figure>
                <img class="figure" src="/portfolio/csci5622/figures/dtrees/dtree.png">
                <figcaption>The resulting decision tree for the data.</figcaption>
            </figure>
            A graph of the x-relative error compared to the complexity parameter is shown below. The minimum complexity parameter value for this tree was 0.016. Interestingly, the resulting tree is just four levels deep, suggesting that many of the variables included in the model were not relevant for classification. 
            <figure>
                <img class="figure" src="/portfolio/csci5622/figures/dtrees/cpplot.png">
                <figcaption>The relationship between the complexity parameter and the x-relative error.</figcaption>
            </figure>
            When this model is applied to the 20% of disjoint data kept for testing, the overall accuracy of the model is 39.17%. While this might seem low, its important to remember that there are 5 categories. The no information accuracy would have been 23.45%, meaning that htis model is nearly a 2X improvement over the null model.  The 95% confidence interval of that accuracy is from 37.3% up to 41.1%, a relatively narrow range resulting from the size of the data. The p-value of this model is nearly 0, so a definitive improvement over null. Below are two copies of the confusion matrix, where the frequency is mapped to size and color respectively.
            <figure>
                <img class="figure2" src="/portfolio/csci5622/figures/dtrees/confmatrixblocks.png"> 
                <img class="figure2" src="/portfolio/csci5622/figures/dtrees/confmatrixheat.png">
                <figcaption>The confusion matrices where frequency is mapped to size and color respectively.</figcaption>
            </figure>
            <p></p>
            Finally, to wrap up the results of this decision tree, below is a list of the variables as ranked by their importance:
            <ol>
                <li>hv208 (Television): 25</li>
                <li>hv209 (Refridgerator): 21</li>
                <li>hv206 (Electricity): 18</li>
                <li>hv243a (Mobile telephone): 14</li>
                <li>hv247 (Bank account): 7</li>
                <li>hv243e (Computer): 6</li>
                <li>hv243b (Watch): 5</li>
                <li>hv212 (Car): 2</li>
                <li>hv216 (# Rooms for sleeping): 1</li>
                <li>hv207 (Radio): 1</li>
            </ol>
            To experiment a bit further with decision tree creation, another tree is created with a manual complexity parameter of 0.005 instead of 0.016 in order to create a tree with more nodes. The resulting tree is pictured below. Interestingly, when this model is run on the test dataset, the accuracy was exactly the same at 39.17%. This is good evidence that these lower nodes are not important for the performance of the model. 
            <figure>
                <img class="figure" src="/portfolio/csci5622/figures/dtrees/dtree2.png">
                <figcaption>A second decision tree model with the complexity parameter lowered to 0.005 to add depth.</figcaption>
            </figure>
            Finally, to be more rigorous in evaluating the performance of the decision tree model, another model is created using 5-fold cross validation. This method is better able to optimize for performance on unseen data by average the MSE for each of 5 test runs. The resulting tree is pictured below. The most striking feature of this tree is its simplicity -- it only uses two binary variables (TV, then cell phone), to classify households. It completely ignores categories 2 and 4 and instead just classifies households into poor (1), middle (3), and rich (5). The performance of this model on the test dataset is 35.8%, just a little bit worse but much simpler. This entire decision tree could be communicated verbally to someone. An interesting final result!
            <figure>
                <img class="figure" src="/portfolio/csci5622/figures/dtrees/dtree5.png">
                <figcaption>A third decision tree model created with 5-fold cross validation.</figcaption>
            </figure>
            The most important variable has changed! We now have "type of floor" emerging as the biggest differentiator between the wealthy and the poor.  
            <h2>Conclusions</h2>
            Creating a decision tree a very interesting exercise for this dataset. One of the most useful insights that we can glean from the decision tree is the list of variable importance. The biggest differentiator between the rich and poor seems to be ownership of a TV, followed closely by ownership of a refridgerator. Effectively none of the poorest households own a TV, and owning one means there's a 50% chance you're in the top 20%.  Owning a TV and a fridge means there's a 2/3 chance of being in the top 20%. This is interesting given that a TV costs around $50 and a fridge around $250 in Sierra Leone, suggesting that the average amount of assets held by a household is quite low. On the other end of the spectrum, if you do not have a TV nor a cell phone, you are most likely in the bottom 20% of households. The best differentiator between the bottom 20% and the next quintile up is owning a wristwatch. 
            <p></p>
            Another interesting outcome of the model, is that is it is much better at predicting the extremes than the middle classes. The class-level accuracy is highest for the bottom and top quintiles. This means that it is easier to distinguish the poor and the rich, but harder to differential among the middle class. 
            <p></p>
            Finally, let's think about why the model may have performed relatively poorly. The columns included in the analysis were selected to be representative of the assets that households have, but the accuracy was still just around 40%. It seems that there are other important variables that may be used in the calculation of the wealth index.  Possible columns that might be relevant for predicting economic class in future analysis could be:
            <ol>
                <li>hv205: Type of toilet</li>
                <li>hv213: Main floor material</li>
                <li>hv214: Main wall material</li>
                <li>hv215: Main roof material</li>
                <li>hv244: Owns land usable for agriculture</li>
                <li>hv245: Hectares of agricultural land (1 decimal)</li>
            </ol>
            Such analysis is recommended for future studies. 
            
            
            <h2>Bonus Tree</h2>
            The future is now! The tree is rerun with the additional variables suggested above because the author was just too curious. Below is the resulting tree--notice significantly more depth.
            <figure>
                <img class="figure" src="/portfolio/csci5622/figures/dtrees/dtree3.png">
                <figcaption>A fourth decision tree model created with the addition of 5 variables.</figcaption>
            </figure>
            The most important variable has changed! We now have "type of floor" emerging as the biggest differentiator between the wealthy and the poor. The next two most important variables are type of roof and type of wall materials. It seems that the DHS surveyors factor these variables much more heavily into their categorization of households in the wealth index. The full new variable importance is:
            <ol>
                <li>hv213 (Floor material): 18</li>
                <li>hv214 (Wall material): 16</li>
                <li>hv243a (Mobile telephone): 12</li>
                <li>hv205 (Type of toilet): 11</li>
                <li>hv244 (Ag. Landowner): 9</li>
                <li>hv245 (# of Ag. hectacres): 9</li>
                <li>hv215 (Roof material): 8</li>
                <li>hv209 (Refridgerator): 5</li>
                <li>hv208 (Television): 4</li>
                <li>hv243b (Watch): 2</li>
                <li>hv206 (Electricity): 1</li>
                <li>hv243e (Computer): 1</li>
                <li>hv207 (Radio): 1</li>
                <li>hv246f (Chickens): 1</li>
                <li>hv246d (Goats): 1</li>
            </ol>
            Another very notable thing about this new model is that our accuracy has improved from 39.17% up to 48.97%, a very significant improvement! Consequently, the confusion matrix has more values concentrated along the diagonal (more red) than before, as pictured below.
            <figure>
                <img class="figure" src="/portfolio/csci5622/figures/dtrees/confmatrixheat3.png">
                <figcaption>The confusion matrix for the latest model with a nearly 50% accuracy for 5 classes.</figcaption>
            </figure>
            A very interesting improvement overall, very worth the extra run!

            <h2>Super Bonus Tree</h2>
            While running another model on the data, it was found that the target variable "hv270", Wealth Index Combined, is predicted much better by the data than "hv270a", Wealth Index Rural/Urban. This makes sense because here the data is not being treated in two categories of rural and urban, but rather all together. With this finding, the decision tree models are rerun to predict the new target variable, and the accuracy improvement is significant! Below is the resulting tree:

            <figure>
                <img class="figure" src="/portfolio/csci5622/figures/dtrees/dtree6.png">
                <figcaption>A fourth decision tree model created with the addition of 5 variables.</figcaption>
            </figure>
            
            The first node separates households based on the material of their floor. Three common options all are earthen floors that send one onto the poorer branch. Next among the poorer half is whether one owns a cell phone. Finally, the poorest fifth are distinguished by the type of walls they have. Natural materials put one in the poorest quantile, and otherwise the second poorest. This tells us that the poorest households in Sierra Leone are well characterized by earth floors, natural building materials, and no cell phone. For those with natural floors but a cell phone, they are distinguished by the type of roof -- thatch and mud roofs being the poorer of the division, as compared with tin roofs. Back on the wealthier half of the initial divide, we have households with wood, concrete, and other non-earthen floors. These households are then distinguished by their ownership of a TV. If you have a TV as well, you are immediately into the top quintile of wealth. Of those who don't, they are distinguished by agricultural land ownership. Counter-intuitively, owning agricultural land puts one into the middle quintile, otherwise you land in the 4th quintile. This tells us that farming is not a particularly lucrative source of income, and those who have other income tend to fare better. Fascinating! The full list of variable importance is below:

            <ol>
                <li>hv213 (Floor material): 17</li>
                <li>hv214 (Wall material): 13</li>
                <li>hv208 (Television): 11</li>
                <li>hv205 (Type of toilet): 10</li>
                <li>hv244 (Ag. Landowner): 8</li>
                <li>hv245 (# of Ag. hectacres): 8</li>
                <li>hv243a (Mobile telephone): 8</li>
                <li>hv206 (Electricity): 7</li>
                <li>hv209 (Refridgerator): 6</li>
                <li>hv247 (Bank account): 3 </li>
                <li>hv215 (Roof material): 2</li>
                <li>hv246f (Chickens): 1</li>
                <li>hv207 (Radio): 1</li>
                <li>hv216 (# of rooms): 1 </li>
                <li>hv243b (Watch): 1</li>
                <li>hv246d (Goats): 1</li>
            </ol>
            Now onto the accuracy. By predicting the combined wealth index, the model accuracy increases from 49% all the way up to 71.08%! Finally, we have a meaningful, high performing model that correctly classifies most households! The full confusion matrix is below. 
            <figure>
                <img class="figure" src="/portfolio/csci5622/figures/dtrees/confmatrixheat6.png">
                <figcaption>The confusion matrix for the latest model with a 71.08% accuracy for 5 classes.</figcaption>
            </figure>

            <h2>References</h2>
            <p class="reference">Chauhan, Nagesh Singh. 2022. "Decision Tree Algorithm, Explained." KD Nuggets. https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html</p>
            <p class="reference">Provost, Foster; Fawcett, Tom. 2013. "Data Science for Business: What You Need to Know about Data Mining and Data-Analytic Thinking."</p>
            <p class="reference">Kamperis, Stathis. 2021. "Decision Trees: Gini index vs entropy". https://ekamperi.github.io/machine%20learning/2021/04/13/gini-index-vs-entropy-decision-trees.html</p>
        </div>
    </div>
</body>
</html>