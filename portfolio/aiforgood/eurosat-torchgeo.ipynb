{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GeoML with EuroSAT and TorchGeo\n",
    "I developed this notebook for Week 2 of a course, AI for Good, that I co-teach with [Professor Zia Mehrabi](https://www.colorado.edu/envs/zia-mehrabi). In the previous week, we learned how to use Pytorch for machine learning with a simple MLP. This week, we're going to use a related library designed to work with geospatial data and models within PyTorch called [TorchGeo](https://github.com/microsoft/torchgeo). The documentation is [here](https://torchgeo.readthedocs.io/en/latest/) and the associated publication from 2021 [here](https://arxiv.org/abs/2111.08872). It is an active community and well maintained repository that is super useful especially for pulling in existing models and working with satellite imagery and geospatial datasets. \n",
    "\n",
    "![eurosat-banner](eurosat.png)\n",
    "\n",
    "In this notebook, we'll be using a benchmark dataset called \"EuroSAT\" that includes 27,000 satellite image patches from Sentinel-2 over Europe along with a label about what the image is of. This is a more complex dataset, and so we'll need a deeper neural network to classify the images. We'll create a commonly-used type of convolutional neural network (CNN) used for image classifcation, [ResNet](https://arxiv.org/abs/1512.03385), and compare the performance of randomly initialized weights versus pretrained weights.\n",
    "\n",
    "**Learning Outcomes**:\n",
    "- Learn the basics of the TorchGeo API\n",
    "- Learn an API for working with satellite imagery\n",
    "- Train a Convolutional Neural Network for image classification\n",
    "- Learn to pull pretrained weights for a model and compare performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Before we start, we'll definitely want to use GPUs for this task since the dataset and the neural network are much larger than we were using before. A ResNet18 has about 11 million parameters and the EuroSAT dataset has about 2GB of data. This pales compared to 1.76 trillion parameters in GPT-4 (about 10^5 times smaller), but we're not OpenAI, everyone starts somewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import complete\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# data libraries\n",
    "from torch.utils.data import DataLoader\n",
    "from torchgeo.datasets import EuroSAT\n",
    "import random\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ml libraries\n",
    "import torch # for model training\n",
    "from torch import nn # for neural network layers\n",
    "from torch.utils.data import RandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import v2 as transforms\n",
    "\n",
    "print(\"import complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Visualize EuroSat Data\n",
    "\n",
    "We'll be classiying the EuroSAT dataset from [this 2017 paper](https://arxiv.org/abs/1709.00029). The images are from the [Sentinel-2 satellite constellation](https://www.esa.int/Applications/Observing_the_Earth/Copernicus/Sentinel-2) from the European Space Agency. It takes multi-spectral (visible light plus some longer wavelengths) images of all land on Earth every 5 days at 30-meter spatial resolution.\n",
    "\n",
    "Instead of downloading the dataset to our computers then uploading to Kaggle, we will use the `torchgeo` API to pull in the dataset. It will download a `.zip` will all of the images, and then 3 `.txt` files that contain a list of the filenames of each images for training, validation, and test data. This is standard for datasets in `torchgeo`. This also means that the images won't get loaded in memory until we actually need them, hooray for efficiency!\n",
    "\n",
    "In this next section, we use the torchgeo dataset/datamodules to download the EuroSat dataset. We also calculate the number of samples in each split and visualize an image or two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Classes: ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']\n",
      "Number of images in train dataset: 16200\n",
      "Number of images in val dataset: 5400\n",
      "Number of images in test dataset: 5400\n"
     ]
    }
   ],
   "source": [
    "eurosat_root = os.path.join(\"data\", \"eurosat\")\n",
    "eurosat_dataset_train = EuroSAT(eurosat_root, split=\"train\", download=True)\n",
    "eurosat_dataset_val = EuroSAT(eurosat_root, split=\"val\", download=True)\n",
    "eurosat_dataset_test = EuroSAT(eurosat_root, split=\"test\", download=True)\n",
    "\n",
    "print(f'Dataset Classes: {eurosat_dataset_train.classes}')\n",
    "print(f'Number of images in train dataset: {len(eurosat_dataset_train)}')\n",
    "print(f'Number of images in val dataset: {len(eurosat_dataset_val)}')\n",
    "print(f'Number of images in test dataset: {len(eurosat_dataset_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # of Images in Each Dataset Split\n",
    "Split    | %            | EuroSAT   |\n",
    "---------|-----------   |---------  |\n",
    "Train    |  60          |   16200   |\n",
    "Validate |  20          |   5400    |\n",
    "Test     |  20          |   5400    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Images and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAAFcCAYAAACqUye+AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAANUJJREFUeJztnWmsJVXV92s659x7u5tuQMBZEFRQoyLG2eAQP6DiPMc4G+IQ4hxnnCUOX/yAs6iIQ0ycp8RZE40CakSjRhNnRRxApbvvOaeGJ7va2+mz9m93Lc658PA+7/+X8D5vb6t27dq1a90667/XWnnXdV0mhBAiooibhBBCyEAKIcRh0BekEEIkkIEUQogEMpBCCJFABlIIIRLIQAohRAIZSCGESCADKYQQCWQgrwU+8IEPZHmeZxdffPG29Bf6es5znrMtfR3a56tf/eqV+/nsZz/b93X00Udn0+k0+3+V3/72t/19hGdn+clPfpI95SlPyU444YRsbW0t27lzZ3bHO94xe/Ob35z985///F8Zr7hmkIEU28r73ve+/v8GQ/HpT3/6/9zsvuc978lOO+207KKLLspe9KIXZV/+8pezT33qU9mjHvWo7J3vfGf2tKc97X97iGIbqbazM/H/N5dddln2xS9+Mbvvfe+bffe73+2N5WMe85js/wrf+973smc+85nZ/e9//974TyaTg/9baHvBC17QG8zDsX///mx9ff1aGK3YDvQFeR1hc3Ozf8HucIc7ZLt3786OOuqo7G53u1v2mc98JnnOu971ruyWt7xl/6Le+ta3zj72sY+h0TrrrLOyG9/4xtl4PO5/Fr7mNa/J6rre9nv44Ac/2Pf7vOc9L3v4wx+efe1rX8t+97vfJV0EF1xwQXbKKadkGxsb2e1vf/vs85///MJx4Sd/OPZnP/tZ9rjHPa6fl+OOOy576lOfmv3rX/9y/Ry2roNf//rX/c/jW9ziFv11b3SjG2Vnnnlmdumllw7e3xvf+Ma+v3e/+90LxnGLML8PfvCDD/77+OOPzx70oAdln/zkJ7NTTz21/zke5j7w05/+NHvIQx6SHXnkkX17eO5h/g7lm9/8Zn+9D3/4w9nzn//87PrXv35vXE8//fTsRz/60eB4xeroC/I6QvDXhZ+lL3zhC/uXdjabZV/96ld7Q3P++ednT3ziEyNf3ze+8Y3sta99bbZjx47svPPO641IVVXZIx/5yIPG8c53vnNWFEX2qle9KjvxxBP7r6DXv/71vVEJ/R6O8IIHwrEe3v/+92c3uMENsjPOOKN/kT/ykY/0Ruucc86Jjv3CF77Q/0wN4w8+vOC/e9jDHpb98pe/zG5+85svHPuIRzyi/xINP1+DIXvpS1968HpXlz//+c+9f/Tcc8/NjjnmmH7Og2G6y13u0hudW93qVnhe0zTZ17/+9f7n9U1uchP39X74wx9mP//5z7NXvOIV/R+n8KzCPd797nfPjj322Oztb397P55gBJ/85Cdnf/3rX7MXv/jFC3287GUv632c733ve/s/DMHg3/ve9+7Ha+dKbDMh3Zm4Zjn//PNDSrnuoosucp9T13U3n8+7pz3tad2pp5668L+FvtbX17vLLrts4fiTTz65O+mkkw62nXXWWd3OnTu73/3udwvnv/Wtb+37+NnPfrbQ5znnnLNw3Iknntj/5+Hb3/5238dLXvKS/t9t23YnnHBCd7Ob3az//9vxH3fccd2///3vg23hXoqi6N70pjcdbAvjCce++c1vXjj/Wc96Vre2tnaw39/85jf9cWGeLXRfhxLmbTabdbe4xS265z3veQfbbZ9hfOHfj33sYzsv4d7Lsux++ctfLrSHPiaTSff73/9+of2MM87oNjY2uiuvvLL/9ze+8Y3+mne84x0X5vC3v/1tNxqNuqc//enusYjl0E/s6xCf+MQnsnvc4x79F1X4EhyNRr0fL3yBWO53v/v1Pze3KMuy/8oKPyH/+Mc/9m3hJ+t97nOf7IY3vGH/03frv/CFF/jWt7512PGEvsJ/V0ecCT9/A+GnYfgiCj+xw09tSxjXrl27Dv473Ev4oqKf5If+bA3c7na3610Sl19+eXZ1CfcffioHl0T4SRzmOfzfX/3qVzjPqxLGGtwghxK+RMPzs1+iYb727dvXf+UfyuMf//h+Pre42c1u1n+Bhl8Q4ppFBvI6QvBTPfrRj+5/XoefW+ElCT9Bg8EJxsAS/FGptn/84x/9/w0/1z73uc/1hvbQ/25zm9v0//vf//73bRn7f/7zn964h5/z4WfrlVde2f8XfjKHF3vLeB5K+FlpCX69IGIMHbvl/6Njhwi+vFe+8pXZQx/60H5uvv/97/fzHHygh+vvete7Xu+z/M1vfnO1rhdcDpbwfKg9/CHb+t89z9oeJ7Yf+SCvIwSjGHxUH//4xxe+FlJ7CYN/MdW2ZVDCSx2+YN7whjdgH1sv5Kp89KMf7b98fvCDH/SigyVsg7niiivwf9sOgshBc0UGJMxz8OeGr8hDCX8s9uzZk7xG+EIPX31f+tKX+i/0IHp5OPRZbhGez1/+8hf0j249N8+zpj8yYnvRF+R1hPAihZ96h75Q4SVIqdjhZ2v4QjxURAjGNQgxWy9vUFCDWhra7nSnO0X/bZeBDF+I4edyGFP42Xfof295y1t6w3XhhRdm1xTh53kwkmED96HQ3IX5tQp0EIz+9Kc/DV4niEPBrfmMZzyjF9Es8/m8/yodIhja8DN7yyBu8aEPfaj/Sr3rXe8a/QE6tDJKcEOEbVRBqBHXLPqCvBYJLwUpwg94wAMObgd51rOe1avQf/jDH7LXve51/U+x4B+zhK+MsN8w/FzcUrF/8YtfLGz1CQrxV77yld5fdfbZZ/cKbfi5HsYQ9iuGjc2H+xI66aST+v97OD9kMMDhyzHsDwzjsQSf6tve9rbeiG539M+hRu8JT3hCr2qHPwbh53IYU1DRLWGeg7J+8skn91/Xl1xySW/EPV+EYdvVO97xjv4ZBTU73HNwVwTDGBTlsP3ntre9bb9t6HAEVX/LPxx2F4QtXeEPSDDUQc0P25kOJfhag7siGOagYofzwx+ELTVfXIMsKe6IJVTs1H9BMQ2ce+653fHHH98rnKecckr3nve856CSeyjh389+9rO78847r1eZg6IZFOwLL7wwuvbf/va37uyzz+4V5XDcUUcd1Z122mndy1/+8u6qq646rNobVNjw3+F47nOf25/74x//OHlMULbDMZdccsnC+C3hWk960pMO/nvr3sM90HxuzVvgX//6V6/qBnV8x44d3Zlnntmrvfa+rrjiin5nwLHHHtsrxve85z2773znO93pp5/e/7fF4ZTxcK9hnDe96U278XjcXy/sNHjVq17VXX755Qv388AHPhDn5NJLL+3HuHv37r6P29/+9tG1tlTsCy64oH+GxxxzTL827nWve3UXX3xxcr7F9pGH/+eaNMBCiOUIG8XDV2YQwLb2toprF/kghRAigQykEEIk0E9sIYRIoC9IIYRIIAMphBAJZCCFECKBDKQQQqwaSXPa4w4fHbBFtKkSYlEz3HoZt+VZfK5t6XLqKz6v6+K2AoaGmOO6JvONv4RxtHBPebvw7xbGX8Bt0q3TuV1j+p/GN9BM51Fb2ZZR2/r6zqitbuJkGpv7F9uaenEMgWkTjwM35canwrKCeaV1RusRzvU8gBwewB7IFn6vW8aJKa5flYNrtIgPyfIsnrNpG0/QvI7HVpj+S7onnLMyaqrKuG1s1ntJcw1j7eCpw3LJalgvZmmjaZnZg0Iy5nd9KPOgL0ghhEggAymEEAlkIIUQIoEMpBBCJJCBFEKIBDKQQgiRQAZSCCESyEAKIUQCGUghhFg1kgYCURAbXEC75DlewneBzhyHPXW+qByKrsHwFNNfBxECOYyE7xLGYZpMYE1yXHQYRRbZynoFhGh0ELUx2x8XppruvSJqKyHSYjQeLf57Eo9rMoUIkLkvUqSL7p7CbQpfpI4nkIaim+C8msYKkRzruw5UYjyUK/cuznfRxKMdQXTWuIhf46Kso7am7QYrLmbO+8x5JhdoKWoM+oegHHwkTUPvcDf47hf0ojjRF6QQQiSQgRRCiAQykEIIsXJdbK8TMvIeoEMN+nc1RSYdk/kg4CN0ZhWy4/X6G9lJCI2Rrwb8WF3hfCSOewLf02gSO4LyYhy17f9P7Jfs5rG/a834yshPtr9qBzPO9GOD9TJvi2Fft9PFnDnOpSxJOYy1hjQ0a5NRfEU4d14v+l8nZfzMc3AIVpSVB46zPsi2hfPgPluaR/C1tvblpLRX5NAEn3hRwtqAgVT2nuidplRYTvQFKYQQCWQghRAigQykEEIkkIEUQoiVRRrHxlA6jJy+rNGQiBIfVxinLG9E912TPPZ2I3p/TU9X6NyGja2eeQRHNktkpGy5dj0DsIF3HDvPNzZi4WZzb1xy4d/1ophTGgEi5cNvYXM0bdK2+gUJDqjB4dpoPbVD4kOoKxAEGtiwvhdKXNjlQn218MxRC8RyJVbhpPOoDEMWUVCwhGkjEZRElGldu8bPZU0W22CZZQ0ISl70BSmEEAlkIIUQIoEMpBBCJJCBFEKIlUUapyntwFkeHUPZdiBShEUUzyA8ByUc2TA2m72npYw5lA6HomYcvmeK0PCGDGGWFSP6YD1wuCZle6l2xs9pDI9uc2992CiRA+PwRb/gc/JkKIIsOigGOrLaFLioCldmo737YhHruN0gds1tAXa678x1zQrfbHMPJGxRWx7PI+hOkYDUriDGslgE0UGFGcg4Xmf5fPnvQH1BCiFEAhlIIYRIIAMphBAJZCCFEGLlkgvezehRZi1f2nXCE4VDjlsSUcgT7I3yifvy7fLfVshh7zsMoh4615xRlihK01VCqrSxucZ0b+w8b+ZwT7AiRya12YH+F5lTqQMoT4CCAAqLdqENHpGMBCLhbMckFmmuuGrvYkNZudZsBfdZghJndSwqRZDD+OcwfprvNmsGxTVKB0fCU0OLD1Sf0pxcUeq0kdKdCSHEtqOf2EIIkUAGUgghEshACiHE6unOloRzTsV4dY+o7ravM3IEO7Wc6EAWmSgqxHmc9ZaT0kJhCRR8RFeMFDan4ANtWC8HxmFTpY2buC7LdA71beCiTR6nw6rNzXMgFtR0oRR0pFYYESKn0BFaU7Co1qpYONicOhZf4XsmNS29JhbFRibqpBpBPe1YO8oK6KuZQVq3xtakobo48bMkoRUFMIjo6UzeuxzeE0qX50VfkEIIkUAGUgghEshACiHEqj5IdKdlw24UzsSCZ0YttDc9zgBDGXmoe0qZT6n2PTfq9dgtWXIBHaZ0SW+t7+F7WqWutGdtlJN4qY1hU/LMZAEKNHBca/yqJfgIKQNPjplpqOa4KR9AfUUtWVZCLesZ1Jagkguj0aKftoIsPbTpvAW/WwfnWld0C5uxyZ/ZYJBFN3hc09SuTfk0/pIyeeWOa9Ix2fLoC1IIIRLIQAohRAIZSCGESCADKYQQK2fzcR9nNlXTZleXEMKbbu253JMv9RCXTaZNq1FNBFf/VL4BFY1ouOSg9mwA51u3w0VxisZFwlZ8FApDhTm3gOwy1Vq8/Jo5bAaeQVskCPjmugDxwpRW3jpy8Xq5r8ZzBZvCKcNPOQKxyFxzDHMWyx4smFDdavvu7JvNXdl2ShCeaCGUVgSiMiQwjyWIRVh1BN72tVE5LDYuXxZbX5BCCJFCP7GFECKBDKQQQiSQgRRCiJUjadxZy4tBIWH53BrxyRw3gooGtNHJnhRCFHlBNZgJR2SRNxII/r7l4JGONSAag1d4gqFhdnyTDYeuCaUCxhsj13HzffOr/9wSmV3KUTEowBTwzKkqQEHfHBA9sgaZdJomH66BTcIQJhqCezJz5NT4shJEJtLEZna+McMSRf34hNwCRmd1rAIEpUrZfIQQYvvRT2whhEggAymEEAlkIIUQYlWRhqIGCBs5Q05xDDCBvjxXJEEGU1NhWnefYz++hE/w8Yo01ltODnZyn5MgQ8KNa/yZN1W9bx1E043TCtccx20jqA/dmJIIzXQ42iYF3aV9Blw9g9KpxYdtQMTQqIKa11ZEwVrl8fMdQS3oFoSh/fPFxF8zqG0NsS8ZiSg1nDudL15zMgJBDESUhrIO4tqGEg5WTMPUb9nS6AtSCCESyEAKIUQCGUghhEggAymEENdaJI1xmlIdYq9I4DmMdv6jeAGpo6huskttcdaMwRrMFL5go4Mw+oguUDlDXZrhyIXMBwlgeO5wKW4WRyA/Xj6KrzA2wscMamzntk5zorYyCRrRIyhBCKHlA20b40nUVkMoihUmqiJ+vjCMrGvjqitXQYq4qUklRzVpOqjP08ITJpEm0l8Kn/BHdYIaihJzZArEZ7nCZ6C+IIUQIoEMpBBCJJCBFEKIBDKQQgixqkjjxaYpoggc3NheOEWOqC+SCCAdWecUc1yDowgHOI1q6sBhNkIAowgo9RXdJw9kOD0ZFYyHnqjWiS/KxDcXOP0wuHK8qFaUpjZJoKYQDbiCCQD57+AWzy3i8i34TEYoBoJgAhEf43K4Jk0Jqd/2bcb9b4KIYrKp4ZqlOStQ1YP0b3kzvIAoVIcEPLgmpS2r7D2RzVAkjRBCbD/6iS2EEAlkIIUQ4tqqi219gs6EKuhnIh+k3TBNGU9orzS6Ibz71a0/hzasQkYV9I96xotOPRgXbSin/u3ma5pXeFBYYxgHQm3Wr+o80VmLu7I7ptfjpdzOY8dhTs5WvHfjS4d5bcBftzaBdYA7nLtB/2JF9aihq4ayV8WHZZX1X1IWHSwr3cb9wwUKs85K2iiO44dnDj7UFoIISvPejan+BPk9negLUgghEshACiFEAhlIIYRIIAMphBDX3kZxK9Isn7bfU1cXne6YrQa9yp5To3Hw6FuXo5kFEnNPzlIHLKKQCGTaKGMOZtuJ+2rpeVKTdYw7BQFvnfPWbuQ2G8cD1Vq8U3m+v477J4HQbuQufGtqxziu613ChmkS0xozSQ2Mq4JsO1hawqFFldAX7EPP8JnDNa0oA3v33V9pVnw5cE1oMpmAKFmWV/T0jk0IIYQMpBBCpNEXpBBCJJCBFEKIa0+k8RzlFCE8jn13WnfoHxUB6m/4PHaUc8Xl6LDBI1gQ8EYpWY3AN6owfBBz6AIwDvtYsDwB1iqnh+6YM1gHFdSjnu9vfGKXDT7qHJEpIaCHRJQmFoZKqPXdGVGmaeKxUnANRdx0XRxFZDXDMZR0GFMZhgwEMBhHJJzFh2D5AzquJvEVlsbclJGooXxDNZdII4QQ245+YgshRAIZSCGESCADKYQQK6c78yoCUV1sOIYb4ybcFe8ouLwSED1ixAqKMKH09d2S6d95rknQcIaiOPCWP/AIGoHW3DulU6MIGYy8IuHMtuXDZRkCI2ir90E6L8enwwiUijUQX3JYHC2VRDDp05xZ0rIayjesV3FEj00N6BVHihLa4N5Lo8Q1oLK2UA+cVt+sjs+tQbSyTSQejT0PM4G+IIUQIoEMpBBCJJCBFEKIBDKQQgixqkhDzmEid9Wt9pyZqN9inf9k4sG7zc5h37lxk7fWzHBNnb43M7nuGfOmceqWFGRwIL6IniigxBlVhPNPZ+bDghitjQJqxnSzOOrE6hL2GQXWqvj1mYzicVDar7yB48waaqHmDUaYwHfOzjUQo8y/O3gn5hB1Mq9jcSQH4cbW8cbaUHFQUdaAyFRCyM0chK24Dep1qyaNEEJsP/qJLYQQCWQghRBiVR+kd69l7GNz+sncvqfFgRRUaNftL6UMP8MbsjnDPWS+IR+Mp2g3Za+h26SSER5nMdWBxkoKXh/nsF8V3Y3oy/WVFLBZf3JKEwNU67FDar63GPRZ55C5ZwKZeypaU86SC2Nb65ueL8zPCN6dCWxYz+aLAwHXX5ZVtDs9j5tmUM7C7NKe2PsJ4yeLAxvAceM/rLORaRvRkvLqD4C+IIUQIoEMpBBCJJCBFEKIBDKQQgixukjjs6XL5tYh53NBZRKME5wS/qDvHzeALzdab9kBFFZc2Y1W2FzPN7/4TxJpHKUgUlekaeSa3Y7eUJAZ3hxNwhlrEFCaYRSvvdps0qb7IRFiHTaiz+ewIbuId0xXZkf5TtiIXoFw00RFNVgZqowA08JpLXwzlU6xMWqCC5Sk9sI80sMbgQBpxVGyGSjkOtEXpBBCJJCBFEKIBDKQQgiRQAZSCCFWFWnI+bksOdhlmw4+WXHBeG+pdra3fxRMUCSwDb7MMaQttFQ6IQo6Wa6edk8B5QPaclCQ4SLG1ETjd6wNrB8Ah9EwMLomHlk8LugLImKKNShPsDk1fcWdrY/i12cH9JXvhfIKcKe2DnZTg2ACNbArEFAbEDTG5rg4h1GWdXOo4V3EYx1TiiIzR7RWCpZV465IpKnia7a2P7jvVYqy6AtSCCESyEAKIUQCGUghhEggAymEECuLNE5XZ+TLxjRXPjwZ/2lcLSg33lLcrgAQt15F0TvL1bLmOfOl349KSENnJBp4oYgkm6IMe/dGMtFhjrRukQM/kRYtqrHdNy72X3WxQLBjI+5rA0SUGTzzCQgftgTCFCJkaig7MIFrchrAZri+OxV4z6kueTwfVv9qQNiiNkpnR4+X6ouvjU10EJV0gBrbXvQFKYQQCWQghRAigQykEEIkkIEUQohVRRpf+qo4LRrVeHGWQ0YRwvppsXusb+NLrRVdgIZBogSHncTjwHozSxaupvAdEKjikiLD0TxJ3CWGiqXqnvOk0QUcc+ZU4XKow2LX0AhEmiMgamYC9a4pLRqJIV1r3h0QZDoQmRoQozqIrimN5jOGyBR6d+olo1OoTFDbQL3rOlZWplCLu4VnFwVGcShWtiz6ghRCiAQykEIIkUAGUgghVvVBUulaIjf52dECo+sJNjiTn8b4SNBdRxl5eKdy3L+zHIEH8nuyr9Lck69c9NXZsW7GRV3BfWOqeqeDNKoiQdl2vM5Wep6eMhW+mulU87owNZ53wKty9K5x1LYOK34NanHT0ObG5zibU2YmeE6QlqcDX7R1j9Jm8nVoq2HtNVl8T52j5EIH7/QmrQOyN/ASzyI/bdxXxS+PC31BCiFEAhlIIYRIIAMphBAJZCCFEGJVkWYdMoYQdlNpm8PGZTqRNnc7/O6Fd2Oxc6M4ZQdqzEBoXCjIeHdfOzaiMzSTjs3juVeQ8Rb25uIYy90S9BXvdM86IwZi91TSgRL3wBqdTBZfjT1lLMjsGcdCxRqIjTsmkHHKbArvx2bmlmrF0zqrQZChzd12oWFta9CTauiJXwKTAQlsxgjEnXwNxJcSNr/TNc19bkLmntbUOL866AtSCCESyEAKIUQCGUghhEggAymEEKuKNGMTWZCiME5Sqs9LukpDYg5m1imG/PeJwA6KkAGRhnbdG+fwKhXCMU7E3mfrTEG/bMUCToEUX9MVwRISIDlrdjvGgWIR6g02ogqiTijLjfOa43Lx1di9M87cs15Alh5akFiyADLTGIFqDu9EBdccwz0tVvU+wMy+mzMIVwFhpYNxUGhXlEEIXiUIWsoqaNy3Gc/PFLL+7NmxKJ5VkJlpPgflyYm+IIUQIoEMpBBCJJCBFEKIBDKQQgixqkgzddaWjdKFobgQO1JLKmMAURV2wKNRbOOn4JyvKd2WM91ZfJAvagYrOmDEjS1cvUJ1Ag7zWa58Bok0dKAjLZo7Ez5qep5xOOa1bwRhDpa2jQLZvSt+VcYQDTOHqI2q9k3Z3NTFxpoFJDbCnI1ohqpqcF6dWQ0zfE/MvVOaQIrKgSoMGIFH6d/m7aIAc8TOSXzN6SxbFn1BCiFEAhlIIYRIIAMphBAJZCCFEGLlmjS0BR6IUzZlrggNrMMCu/UnxWJEwwhSTtXTOI6gphrDzlrTUQlmZyiNJ10bd5g7I4EcXVF3GElDNWN8Nc1ZRBmuHZSBIx7rhnNhbHOQL2ILBTCK+DDr/ci1ON3ZiLJvwT1N51AnBepW20dAzzwjYYhqv8RnZiNTn7vA6KnCFb3Twfg3rUAFc93Ac6J0ZOujOHKp3QVtxXhwfopCIo0QQmw7+okthBAJZCCFECKBDKQQQqwq0lAECGIcv0Xl85TXjc/5v5kvVkmfz+K9+ZRiDUePjn2KGDLpzlBxAKc7X3R4HHQiloKhej9Ux2S4K84bBzijX6I5w6gW3zU5650n4gnaMGIrPnA8XmzbXcUCQWmiOPo2EGlQIIR1Zt8xqpWDaw/a6JpWIClBaBlTtr8ivs91iGDbMJE6LYyB5JJZS/E18TV3mf4D86j20XB9pKuDviCFECKBDKQQQiSQgRRCiFV9kG4XpOdAzHIDmUXQt2L+7UwHz6Nybi5e8p4w4wn4zqLewSfm2izt/YvnSxLj3NTOHUb+Y/TtUv+UYYmGMVyrnOeCfJDxUUeMF32Ou/FVoZII8VWpLDP5CCemKDVVOZnBS0Ez29A81ovnNpDGKB/BmmrBnwkbskdGbyD9oQDf4jif+Op/w0RuGvGipYfZqeSCEEJsO/qJLYQQCWQghRAigQykEEKsKtLkblu66JhtnXWxMSMMbVptHfV5KRuOe/8oZZMxzmc4i0ousHDjKAOAgoavPASWXLDnwqZw96Z291FRnQfXme6a49E68IkXuOkcrnrEZFGkyUkxJJED1iOJLZtGMAnUxeKG6bUifj1JhKihL6xlbftC8ShzZS2q4b22ZR5yEGRQPIJMYaMy3pi/YwzHmUxG+6AsQztdDC65OugLUgghEshACiFEAhlIIYRIIAMphBCrijSUmQMxTmSnb97n6KcWrHftvCgoE85SyvF53jIMKEzY8gTuItjxUd50L57unT1xBpXhv72deyDDoT8kfmHJAoo+AuVmzRxXg0izYWpnp65ZQXRKFlcFyaYmUmRj3DrvCSJ6aG2bYtwtlT/ofBmiahBpbJRPDS/FJogoZQNiDkzZZGcs3KyZKJk5CJBV5bRdgL4ghRAigQykEEIkkIEUQogEMpBCCLFyujOKJACsr5wCaUjR4MxgFNVy+H+nWvkvAY2DhA8bIUBCgi9FGdaajiou+FJ+uRPJu7K1LZ/6jQJ/WLixx8Al3SnWbF++2SARawyhLjuM859EyhKiPcYwjobKMDjmlt4dSgNWGvEltd4LI3bVlGKQ3p02c6ZYW2xtu8JVI3wC15zncQ2Wzc1iMN0ZzfVOUw/86qAvSCGESCADKYQQCWQghRAigQykEEKsLNJ4wkkArqQSt7bO6BHbkoMjuAMXsrMEM+aA8tTZ8cevYIHrxfNwrh1RRX2jQ+DxlZXhK3gDqsxFXLWKUmPzREtRHjOYixZuYAPqPq+ZUI4OojEgS1dWklgE0R2UjqwwDyGHui/0AGyETN8XLPjKDLhoIEIGxMYcNI7CIZStwbw248z1blL0zhxysdWmpvZ4FF+ggjRpXvQFKYQQCWQghRAigQykEEKsnM3Hm67GAWde8frYHA41zFLiLMPgcP/huMAPBHtdeQN1dE/Dm8n5PD7XHkY+H/JZYeEH8rE5xoF9eUtGwNxGN+Hbu5910NdOcLJZl+MEHI4VDKuiN6omfyA49sx4a9iBT8+OtkGjX7JwbJrHDfd53BccZksu0DEbMGk13NS0i1+eOegNtsTFDrjohjaKCyHE9qOf2EIIkUAGUgghEshACiHEyiKNs2JxvHHbt7OYRBpOeGIVE2fJBecGZLdA4uiLaiSTCBGLVstvqnad6bztfBtPLpxzzSU6HCIB7hP3lWE4ehxvLt4YLb4aE1MnOzCqSpc4kpXxOzABsaIz6Xs4G5Rz/oHWbFjHgArKFpRlrgxCbbO4absF6wK6JT6TEsZRWZWp/8JbPHcM5S0mVJjcib4ghRAigQykEEIkkIEUQogEMpBCCLGqSOMOpImc585U+E7FIRJzONzD04QRPR7lAyOBIGOLN9LFc0SHYtRw1Ezq3Pg8X71oLKGB9SCWy6Di7sqO17lAxxARc/RkLWrbOV58NdZAkBlDthqK1KGSFHSujVZr4fmSyEE1sDF4xExRU0M2H4xMa6Gz4bHlMypTEV9zBOILlRLnmuz2oHiGuhW+A/UFKYQQCWQghRAigQykEEIkkIEUQoiVRRp/FWZ7orP2dOuKRLFOfG/UjFcEcoko3mgGZ63sWNgaHMKBw5zO88xGN2FKMboAiV2+mubWsY+lMTwiXEq0siUdqB41lM84wtS7DlxvPIna9uyqBssyZLA+xyNf9FRbQzkCK6LAxM6goDZoHNkaiCHRbEDqvQqzzRVRWwltdl3NYX3SeSOIKqLgl/lioM5/r2mig2CdzaAWtxd9QQohRAIZSCGESCADKYQQCWQghRBi5brYXhz+UA6yWDLiBj34PnGEhAOsOxyH0sT9e//UUChKNDRnfR5KF4bKRzv8jEDv4SAoX140yvrl6Z/SoqFAaBZRAYuqhriTXU0cNXPEepzKbM/a4qtRgUCQgzoyxhR3cduUlBWzNqgmDdWyKUFZIWHFRkbRMfRQRs60blYUq+BZNnBPVC/K1gjv+yMhzkTO0Hs+nVP8kQ99QQohRAIZSCGESCADKYQQq/ogl03m4+3Nnd4/8ldA2njO2w9ttNF6eMN663Pd4CVtivj0ydEoHMfg3uX4mG2sNBEocGzD6f35LN/asL4mykaUw0bxI8u4vMKu9XjS1kyt7BbrmdM90QOgTEnxcY0pWTADv2cB84g1tmGjeFkuzu0IfHqU+amCrE6UIac1Lwb1VcKcdU07uGn+QH9UDsLUXwcfPwaJONEXpBBCJJCBFEKIBDKQQgiRQAZSCCFW3yjudHTaTPjgoG5JHMGaCDSMbrmCzpjLnwQeGG/nyMiDG8CdGZActb7pmiT48Ib44XG4Hdl0m+DEh4OgL+emcOrOHNeAo7+Yxm3HHL0etR2xI34N8qkt7O1bZ5RBqK5JhIBMPaapbhuXrNhAthoKgli366CM73tEG8AbKGMApQ1Ks45rzAblK0nRUnYvrKNeDK4DyqbkRV+QQgiRQAZSCCESyEAKIUQCGUghhFhZpHHWtwbZw+ls9V7TKiZYXwGga5LI4cF3lD8Djy0fQGdRjWrf+Isla0hjViHU0oajX1CvckYf4RoyYsJs/zw6ZmMa//0/ane85NdIc6uNCAH1tGl9zkGk2QSRZuoRc0C8IHHHRuAEqiK+z7F53QsQQuZQ0mEO4ygLKqdgMiyBUET14+dUy5quCUJfbYRWek+yRnWxhRBi29FPbCGESCADKYQQCWQghRBiVZGGU1rFWB8yR2hgsWw4bNkd8D4RiFJTUcr/qLsVomYwCseOrsFaBHCarwZzZ87lafVG/XhlrMWxUbYwvCdn/fLOiBz1NBYq9uQbUduRR8bpzrI6vsDMCAfjfORaK1PI0zUHtaUFkaZxpPeiFG60HGdQZmBmanubjG49NYpHXdRWUWSReZ4VCDm0fEh4KqEOOaVPa42C1MG4SkXSCCHE9qOf2EIIkUAGUgghEshACiHEqiJNu2RdBz4LIiNQt3FEyThDNHIKSyDhAIjqb3idvm7hyRHpghniwGGPqdhM/RbqH4flPc4RpUTRQRQuQX+zIfqiMQVbcpsrLKQ2270jatuzG8SW/bReTBuk0cLUY6CYUHQKHWcntwDBoaO62yAMgdaSzUzashZEPhsNEyihvg3Vt46Xy3DUWN9/1MI3MIf7tDof1vr2pONLoC9IIYRIIAMphBAJZCCFECKBDKQQQqwq0jiznYEw4Yyk8ea+sk5kpxBChyWSg8FRjSPYY4XaOC4BzPe3jNI9uZKirZKOzCPweKN3SPCBqJDGpDdbB1f/9fbEUTN7xpAGbAZDaxfnu8a6LPFpc0htRtEpFGFWGlEGawl1TuEDa+jkgyLTGM6bjGJhq27rwec5ghRxeTyN2f4uTlVXwzOnelFZUQyKNCRYedEXpBBCJJCBFEKIBDKQQgixqg/S64K0R3q3aPKm8GG/nrcEANZlpnE4NkdTbV8CjyIfkv03Zu5xXZJdidavRJvOV/hTyXv1rT+NggPiE2tIv9/si/1dnfH17R6tRcccfVTcdgRkiSlL8NKajEpzcLnRXm/K3NPBRnfElq32uSCzGgbiqe9e0DqDhVDAQNar2HRMzTMZF9RXfMlmHjdOoSZ4nO8IyolQ5iRylzrRF6QQQiSQgRRCiAQykEIIkUAGUgghVq6L7U7Jv+zGYu84uqXqaWMGG8zlT92Za8IxhbeitkfQwMmgzcaUhQayvdgG2kRMw8JN+L4585xHyXzme+NNw/ksds5PjHCwo4yX8u49cdsIfP8dDMQKfVSDHBL8ZLWzTjsJiXNz7lo5cmVAwv38FCxh/01ZkqD/uoonrRzFwsrYZP0pnEuFNnLT3JKAZBMN0XtIIpYXfUEKIUQCGUghhEggAymEEAlkIIUQYnWRxunotOn9l+8pkfLfoQKR2W9XiAqx10TnM4gjFD3iyK2D9ZC9Y+XwnWwpnGUwWP8azro02xen0en2xmEP61UsCFTGYb82jgWN3RtxWzePb2qzHo5Esdej2tmBgrJBlRDdAeUD7GMiYYiEjwoiVhwVHbIGHjBMTzYhMa0mtcv8EyJwSlAp56DIUBxNhe9FPpg5iaKbvOgLUgghEshACiFEAhlIIYRIIAMphBArl1zwHmc89iRKMN4Ijagw9tLdczGCYYEEN/njRUlAgv6tM94Z9MMRN8OjwAgZrGHslNhIOTBay3R/PVg2IbBRxWUSdm3Ey7TdXHwKa7tiQebItTjdWbafRCYQQ8x8TEZQG5puu4nvs6W0X6gttoPqV47RJFRHfTjtWpQG78BRUUvtqLHd92fmbAyiio0W6ocKbRQJNCNhy1yjwggiRdIIIcS2o5/YQgiRQAZSCCESyEAKIcTKkTT+wtjmNIcocaAVmuiajUMx8dWo9rtujfDkqJWTHIejJLW3hDTPIs33cp1xbR9fXeb5vkUBpjb/DkzycjCNWaAgEcU8g917IJImi9s2m/1RW+NYjxQVMuogagbLtENESQXRL0b1wTLQNr9X/xLDRaEOiw1YyUncAfaDIEMvng0YKsr4+VLd6moEz7eNz21BeGrN4q5br9joQ1+QQgiRQAZSCCESyEAKIcTqdbF9/orc49tylkTgK5pzcbOrz0fodMFAMW46xJlViNw59h68TkjEkcYFXZCUfh82KoNvq96cDdayphrPJW0khl3JlKFl0i36qI46It5gPoaxbmLJhbitsb70eeerd13FbRXVqKZMOmYjNL1zdgP7fw+Mx1YMH0bzX0BbSwEVMGcz4+Rsp1BPe+z02zpXdmMyMdFaobXnRV+QQgiRQAZSCCESyEAKIUQCGUghhFh5o/iSGoG3tDK5YF2XpIwnnW8zM/q7afNvpObQWMG57XQ+u5Qt6CyHLDE57Zx3aEC4tx5EjvlV06it3qwHHeNjWGodONRnJDjAODqjyWxAXex6HisyNShz1L919rcw/zNnwAPOLazRwuy0phrbKEKAUFnSt091+M32/XmoAXVR23Qej2PTiEwFlKQY0Zot4/5LeK/nULPblmtAcQc213vRF6QQQiSQgRRCiAQykEIIkUAGUgghrq2SC56gE05pQ7aaMtO0gxlnMHmHM8NPBo7gONuOT7HCoyh8xww4hwgNTgzkfSp2bsH5P4uVivoqKJMAggzdZ2GEAwo0iuWeIC5kLoFtsmPxyB0diEBQb5mEM8rkb4+D0tlZQ/2DRtZShiLH0oPqDVkONbYrmDUK8rFiDkUCtTiwPGqawxqykVckFNUwZwUsDnpO2GbuaUTZghRJI4QQ249+YgshRAIZSCGESCADKYQQK6c7c+oBVjjowGnKCcocaboChSNqBge7XA3pA5fMB1NCdRTBgqUZqO52N5zCDVUacm6D471e9PbXUKO6JfGFoj0y53yb6ZhDnjesQQ73XkH0xbG7Jgv/3lX4BAEMfiHsOCjqh/qnriA6awrnTuf14FhbKFPRUnQQPSdSORwiWQPjz+HbKjfPuG3jZ74f3pMxPHOKGGrgXLse6ZVr3GGAMfqCFEKIBDKQQgiRQAZSCCESyEAKIcTqIo3T0Wkd9qx6uBpJ+MiXjJDBoBk8jsQQ4wiGvjASiMQL8LzbGj3sYAdBAOq3zDfj+tPNdNFZXsydJXWg0dZqOTC2+NzORm2AkED1rqnM+Qie8e7JYr6zCoUt7/zDfRpBY78RUAKQ8SsrKBVeDnV2oDZObcJ1yrjMTlbBXFAkED0U2wK6FhuEnJroPvPBWjYZCDezulgquilgM6V1zjXlRV+QQgiRQAZSCCESyEAKIcTK2XxoByniyO8PWUQ6SLvOm9NNlhivH5F6IucEXdRmKFrhzwple2lMaQD770BHbZBihjLYFM3iDVTk73Vurs/zxrV5PDe+ILpv8qe1tDbAf7meLy7dMaRsIR9YC/WtGygNYKeRxkX+tAbrifh2p08ipyD5UGnDOmQ7gjID9r3ATf/ko20dWgC04boAx2fTwHqEubU+zv82LvyzJd1ihRdWX5BCCJFABlIIIRLIQAohRAIZSCGEWFWkaWlnK0EbdqNDhp36BxrBftuNoaS0tF7Bwff3wTquwaePdZNJMGlm8clzk12HHNQliAQ0jwXt/jUCTI61xOPTWngmBcxZCQKJnTOaHxYh4qYSBIed1eLSrSo4EdYsZo1y1G7PoWwC1dOm+yzhmVQkIJl5pLrYU2ijDEi83s06hoUMyzMjkYbKGNikPyiq0DcZvMMNrVGYx9LMY4vZspTNRwghth39xBZCiAQykEIIkUAGUgghVhVpNv+96TrOCh+2PnKgLCEsB9PG02GL/eeQ8aQF5zn5aUnPIMd7bQsUt8uLNB1k0onuFBzNYxgsOa0p883cDJj0MIqgIOd5VcTPrgThpjFCE3YPnv4c5nEC62XneDQs/OEzh/XoyJ5UUHoZnEdaHBApAqmkZuY5scgxXN4iMAdhZWSFJipPQJmNurhtjdbeaLi+O2YGgpIObUFhbj4bER3jrRcD6AtSCCESyEAKIUQCGUghhEggAymEECtH0uzzJS7PjdgCmYyyOSgV3pIINlKkmoJzvvb1VYPzlgIyCuNkr8ER30Ab7uAHEaIwf6dIw6ogmqSqwFEO15yb2gDk/+Y0Xb5a3DQf9p4KKrlAggk8k/ViFLftWFy6ayAUUdRPUZJ4ASLKrBsspVDCPVFpkhwEE8i6FqVd47rkIGigCAHjNWJIUcFCgxR6OX1G0btTVIPjsqUsDozUJ76QANbY+QBhCB6vG31BCiFEAhlIIYRIIAMphBAJZCCFEGJVkcZZViPy3ZIgANm8MJKGRA4rmJCFJ0c/qTRQ5iUrILqjNBEIOUTDOMrnHGjKh53IaxCtQjVGKHoEh2EOg+4ZUNgoa1nbDAswlJGLhA9yspMYtb62eBNrRiDooYAtrOENp5r1aMXHflwkiFG9a4xOAVHPTG4D85o7nx2ujeFAGozKqUyETGCEothwXewGBKuaUr/RcyIBLB8WZEgM9KIvSCGESCADKYQQCWQghRAigQykEEKsKtJQJAphUyOhBcaUR+SApQPbwfM6qtVCIgGJORBJ0Di824WvxjumBrOCAIkGdE81OfGr4YL0c4x8iSlgbhtwqPMzMNE7VE+E0m2Bd76bZIOiVQHiTgs1V2huqfZOZxYMLcWO0nllVNSF0qkNp08jkWNuU+8lUtBhKjlbm4iibSCUrIAFX5bVcBTdLDoka5zCJQkrmDLPtsE7Te+OF31BCiFEAhlIIYRIIAMphBCr+iC9WcttenP2Tzn9EJTlwzQ1lPEEazD7fIQzR91k2LecVbCDl/IfVeB4G1l/C/Q/q2tXmYcx+DjtM+CSC76/nlHa/oRf1d49PRLKtkMJZnbvjJfpUdYH5ixPQKUlGvA728wxHTzNxrnBGVcCZvMxx8Gkzaj0A2zkrsDZWlofJ2WWgnlswQdJGZy6erEN40HAt9hAaRLMLYV+SfOcfEnH3OgLUgghEshACiFEAhlIIYRIIAMphBCrijTkUCc645rFFPGQUmUdNp7OoC7z2Lhva+MY7vsnB7hD8OnPjZuywhwIl0SwLjOJUWbTMzmopyAkUEr7DnbitmYeKxgDzQVt6qW90RujWFkZV4vPczqDTdvQ1671tajthrs3orad2WIZBk5eQwIerZdhMYfGiplpQHAoYdP2iIIBzKk1CZx0HsghNQpPi89pMoZj2vhZ7odC87P5cECFXQOptU0b4qm8QgFCqD3TWx/di74ghRAigQykEEIkkIEUQogEMpBCCJEg7yi0QAghhL4ghRAihX5iCyFEAhlIIYRIIAMphBAJZCCFECKBDKQQQiSQgRRCiAQykEIIkUAGUgghMuZ/AKoMxLkYnSZ7AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize just one image\n",
    "fig = eurosat_dataset_train.plot(\n",
    "    sample=eurosat_dataset_train.__getitem__(1),\n",
    "    show_titles=True\n",
    ")\n",
    "\n",
    "# modify this code to inspect a few different images and get a sense of the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Evaluate a Model on the EuroSAT Data \n",
    "\n",
    "In this section, we train two ResNet models on the nonspatial EuroSat dataset: one with randomly initialized weights, and one with pre-initialized weights from the SSL4EO-S12 project (use the pretrained weights available in torchgeo).\n",
    "\n",
    "The flow of this section is as follows:\n",
    "1. Configure the GPU\n",
    "1. Define the train and test loops\n",
    "1. Define a ResNet model with random weights, customize the model for EuroSat data\n",
    "1. Define a data sampler and loader\n",
    "1. Load and transform the data\n",
    "1. Train & evaluate the model\n",
    "1. Repeat with a pre-trained ResNet\n",
    "1. Report results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure GPU\n",
    "\n",
    "This should output `cuda` if you activated the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "# set seed for reproducibility\n",
    "torch.manual_seed(44)\n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure CPU for Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see how many processes can be run on the CPU\n",
    "import multiprocessing\n",
    "pool = multiprocessing.Pool()\n",
    "pool._processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Train & Test Loops\n",
    "\n",
    "These are pulled from the pytorch getting started tutorial. I've added a comment to every single line to explain what the code is doing to help demystify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loop defined\n"
     ]
    }
   ],
   "source": [
    "# Define the training loop function\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    # Get the total number of samples in the dataset\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Loop through batches of data provided by the dataloader\n",
    "    for batch, sample in enumerate(dataloader):\n",
    "        # Extract input data (images) and corresponding labels from the batch\n",
    "        X, y = sample['image'], sample['label']\n",
    "        # Move the input data and labels to the specified device (e.g., GPU or CPU)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        # Forward pass: Compute the model's predictions for the inputs\n",
    "        pred = model(X)\n",
    "        # Compute the loss between predictions and true labels\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation: Compute gradients of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # Update model parameters using the optimizer\n",
    "        optimizer.step()\n",
    "        # Reset gradients to zero to prevent accumulation\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # print another period after each batch to track progress\n",
    "        print('.', end='', flush=True)\n",
    "\n",
    "print(\"train loop defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loop defined\n"
     ]
    }
   ],
   "source": [
    "# Define the testing or validation loop function\n",
    "def test(dataloader, model, loss_fn, val=False):\n",
    "    # Get the total number of samples in the dataset\n",
    "    size = len(dataloader.dataset)\n",
    "    # Get the total number of batches in the dataloader\n",
    "    num_batches = len(dataloader)\n",
    "    # Set the model to evaluation mode (disables dropout and other training-specific layers)\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize variables to accumulate total loss and correct predictions\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    # Disable gradient calculations for efficiency during testing/validation\n",
    "    with torch.no_grad():\n",
    "        # Loop through batches of data provided by the dataloader\n",
    "        for sample in dataloader:\n",
    "            # Extract input data (images) and corresponding labels from the batch\n",
    "            X, y = sample['image'], sample['label']\n",
    "            # Move the input data and labels to the specified device (e.g., GPU or CPU)\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            # Forward pass: Compute the model's predictions for the inputs\n",
    "            pred = model(X)\n",
    "            # Compute the loss for this batch and add it to the total loss\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            # Count the number of correct predictions (highest logit matches the label)\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    \n",
    "    # Compute the average loss over all batches\n",
    "    test_loss /= num_batches\n",
    "    # Compute the overall accuracy as the fraction of correct predictions\n",
    "    correct /= size\n",
    "    \n",
    "    # Determine whether this is a validation or test loop and adjust the output prefix\n",
    "    prefix = \"Validation\" if val else \"Test\"\n",
    "    # Print the accuracy and average loss\n",
    "    print(f\"\\n{prefix} Error: \\n Accuracy: {(100 * correct):>0.1f}%, Avg loss: {test_loss:>8f}\")\n",
    "\n",
    "print(\"test loop defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Data Transforms\n",
    "\n",
    "The EuroSAT images are 64x64 pixels, but ResNet50 typically uses 224x224 inputs, as standardized by the [ImageNet dataset](https://www.image-net.org/). To deal with this, let's resize the images using a Pytorch transform before the are run through the model. We'll need to define a custom transform class to handle the torchgeo dataset format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data transform defined\n"
     ]
    }
   ],
   "source": [
    "# custom transform function to handle the dictionary structure of torchgeo dataset\n",
    "class CustomTransform:\n",
    "    def __init__(self, transform):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        sample['image'] = self.transform(sample['image'])\n",
    "        return sample\n",
    "    \n",
    "# Define transformations for the dataset to get it from 64x64 to 224x224\n",
    "transform = transforms.Resize((224, 224))  # Resizes the images to 224x224\n",
    "\n",
    "custom_transform = CustomTransform(transform)\n",
    "\n",
    "print(\"data transform defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Dataloaders\n",
    "\n",
    "Just like last week, we need to create datasets and dataloaders within Pytorch so that our model knows how to iterate over data when training. We'll also tell the dataloaders how many CPU cores we have to maximize data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataloader defined\n"
     ]
    }
   ],
   "source": [
    "def load_eurosat_data(txs, batch_size):\n",
    "    # reload data with the new transform\n",
    "    root = os.path.join(\"data\", \"eurosat\")\n",
    "    dataset_train = EuroSAT(root, split=\"train\", download=True, transforms=txs)\n",
    "    dataset_val = EuroSAT(root, split=\"val\", download=True, transforms=txs)\n",
    "    dataset_test = EuroSAT(root, split=\"test\", download=True, transforms=txs)\n",
    "\n",
    "    # define a sampler for the EuroSAT dataset\n",
    "    # it's a non-spatial dataset, so we can use a regular sampler from pytorch\n",
    "    sampler_train = RandomSampler(dataset_train, replacement=False) # start with a small batch\n",
    "    sampler_val = RandomSampler(dataset_val, replacement=False) # start with a small batch\n",
    "    sampler_test = RandomSampler(dataset_test, replacement=False) # start with a small batch\n",
    "\n",
    "    # define a dataloader to iterate over the dataset\n",
    "    # Use single-process data loading on macOS to avoid pickling errors\n",
    "    # (multiprocessing spawn can't pickle locally-defined classes like CustomTransform)\n",
    "    dataloader_train = DataLoader(dataset_train, batch_size=batch_size, sampler=sampler_train, num_workers=0)\n",
    "    dataloader_val = DataLoader(dataset_val, batch_size=batch_size, sampler=sampler_val, num_workers=0)\n",
    "    dataloader_test = DataLoader(dataset_test, batch_size=batch_size, sampler=sampler_test, num_workers=0)\n",
    "    \n",
    "    return dataloader_train, dataloader_val, dataloader_test\n",
    "\n",
    "print(\"dataloader defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Train, Val, Test Function\n",
    "Instead of calling training, validation, and testing functions for each loop, let's define a standard train/val/test function to wrap them. In this loop, we \"baseline\" the test and validation data before and after training to see how we're improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def train_val_test(dataloader_train, dataloader_val, dataloader_test, model, loss_fn=nn.CrossEntropyLoss(), lr=1e-3, epochs=5):\n",
    "    # record the start time to analyze GPU efficiency\n",
    "    start_time = datetime.datetime.now() \n",
    "    print(f\"Training started at\", start_time)\n",
    "    \n",
    "    # create optimizer from the model and the learning rate\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # print hyperparameters\n",
    "    print(\"Hyperparameters -- Batch Size:\",batch_size,\"Learning Rate:\",lr,\"Epochs:\",epochs)\n",
    "    \n",
    "    # baseline performance on the test data before training\n",
    "    print(f\"Calculate Baseline Performance on Test Data Before Training...\\n-------------------------------\")\n",
    "    test(dataloader_test, model, loss_fn, val=False)\n",
    "\n",
    "    # run train / val loop\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        epoch_start_time = datetime.datetime.now()\n",
    "        print(f\"Epoch started at\", epoch_start_time)\n",
    "        train(dataloader_train, model, loss_fn, optimizer)\n",
    "        test(dataloader_val, model, loss_fn, val=True)\n",
    "        epoch_end_time = datetime.datetime.now()\n",
    "        epoch_duration = epoch_end_time - epoch_start_time\n",
    "        print(f\"Epoch training and validation lasted for\", epoch_duration)\n",
    "\n",
    "    # test result with test data\n",
    "    print(\"Test Results\\n-------------------------------\")\n",
    "    test(dataloader_test, model, loss_fn, val=False)\n",
    "\n",
    "    end_time = datetime.datetime.now()\n",
    "    train_duration = end_time - start_time\n",
    "    print(\"Training completed in\", train_duration, \"!\")\n",
    "\n",
    "def evaluate_model(model, dataloader_test, loss_fn=nn.CrossEntropyLoss()):\n",
    "    print(f\"Evaluating Model on Test Data\\n-------------------------------\")\n",
    "    test(dataloader_test, model, loss_fn, val=False)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a Function to Save Down the Model\n",
    "We'll output to the kaggle working directory in a subfolder called models. Each time we run a model, we should name our \"experiment\" so we remember what hyperparameters we were using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def save_model(model, experiment_name):\n",
    "    if not os.path.exists(\"models\"):\n",
    "        os.makedirs(\"models\")\n",
    "    model_weights_path = os.path.join(\"models\", experiment_name)\n",
    "    torch.save(model.state_dict(), model_weights_path)\n",
    "    print(\"Model saved to\", model_weights_path)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Model\n",
    "\n",
    "`torchgeo` has a nifty API for pulling in some standard models, like ResNet50. Let's import the model along with the pre-trained weights that are available for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported\n"
     ]
    }
   ],
   "source": [
    "from torchgeo.models import resnet18 # import resnet model from torchvision\n",
    "from torchgeo.models import ResNet18_Weights # import pre-trained model weights for resnet\n",
    "from torchsummary import summary # for viewing a condensed model\n",
    "\n",
    "print(\"imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]          40,768\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "          Identity-7           [-1, 64, 56, 56]               0\n",
      "              ReLU-8           [-1, 64, 56, 56]               0\n",
      "          Identity-9           [-1, 64, 56, 56]               0\n",
      "           Conv2d-10           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-11           [-1, 64, 56, 56]             128\n",
      "             ReLU-12           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-13           [-1, 64, 56, 56]               0\n",
      "           Conv2d-14           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-15           [-1, 64, 56, 56]             128\n",
      "         Identity-16           [-1, 64, 56, 56]               0\n",
      "             ReLU-17           [-1, 64, 56, 56]               0\n",
      "         Identity-18           [-1, 64, 56, 56]               0\n",
      "           Conv2d-19           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-20           [-1, 64, 56, 56]             128\n",
      "             ReLU-21           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-24          [-1, 128, 28, 28]             256\n",
      "         Identity-25          [-1, 128, 28, 28]               0\n",
      "             ReLU-26          [-1, 128, 28, 28]               0\n",
      "         Identity-27          [-1, 128, 28, 28]               0\n",
      "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
      "           Conv2d-30          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-31          [-1, 128, 28, 28]             256\n",
      "             ReLU-32          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-33          [-1, 128, 28, 28]               0\n",
      "           Conv2d-34          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-35          [-1, 128, 28, 28]             256\n",
      "         Identity-36          [-1, 128, 28, 28]               0\n",
      "             ReLU-37          [-1, 128, 28, 28]               0\n",
      "         Identity-38          [-1, 128, 28, 28]               0\n",
      "           Conv2d-39          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-40          [-1, 128, 28, 28]             256\n",
      "             ReLU-41          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-44          [-1, 256, 14, 14]             512\n",
      "         Identity-45          [-1, 256, 14, 14]               0\n",
      "             ReLU-46          [-1, 256, 14, 14]               0\n",
      "         Identity-47          [-1, 256, 14, 14]               0\n",
      "           Conv2d-48          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-49          [-1, 256, 14, 14]             512\n",
      "           Conv2d-50          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-51          [-1, 256, 14, 14]             512\n",
      "             ReLU-52          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-53          [-1, 256, 14, 14]               0\n",
      "           Conv2d-54          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-55          [-1, 256, 14, 14]             512\n",
      "         Identity-56          [-1, 256, 14, 14]               0\n",
      "             ReLU-57          [-1, 256, 14, 14]               0\n",
      "         Identity-58          [-1, 256, 14, 14]               0\n",
      "           Conv2d-59          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-60          [-1, 256, 14, 14]             512\n",
      "             ReLU-61          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-62          [-1, 256, 14, 14]               0\n",
      "           Conv2d-63            [-1, 512, 7, 7]       1,179,648\n",
      "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
      "         Identity-65            [-1, 512, 7, 7]               0\n",
      "             ReLU-66            [-1, 512, 7, 7]               0\n",
      "         Identity-67            [-1, 512, 7, 7]               0\n",
      "           Conv2d-68            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-69            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-70            [-1, 512, 7, 7]         131,072\n",
      "      BatchNorm2d-71            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-72            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-73            [-1, 512, 7, 7]               0\n",
      "           Conv2d-74            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-75            [-1, 512, 7, 7]           1,024\n",
      "         Identity-76            [-1, 512, 7, 7]               0\n",
      "             ReLU-77            [-1, 512, 7, 7]               0\n",
      "         Identity-78            [-1, 512, 7, 7]               0\n",
      "           Conv2d-79            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-80            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-81            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-82            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-83            [-1, 512, 1, 1]               0\n",
      "          Flatten-84                  [-1, 512]               0\n",
      "SelectAdaptivePool2d-85                  [-1, 512]               0\n",
      "           Linear-86                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 11,213,002\n",
      "Trainable params: 11,213,002\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 2.49\n",
      "Forward/backward pass size (MB): 74.28\n",
      "Params size (MB): 42.77\n",
      "Estimated Total Size (MB): 119.54\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# pull in a resnet\n",
    "# use randomly initialized weights\n",
    "# modify the ResNet model to have 13 input channels (instead of the standard 3 for RGB)\n",
    "# modify the ResNet model to have 10 output classes (instead of the standard 1000 for ImageNet)\n",
    "model = resnet18(weights=None, in_chans=13, num_classes=10)\n",
    "\n",
    "# summarize the model for the given input size \n",
    "# image input size is (num_channels, width, height)\n",
    "input_size = (13, 224, 224)\n",
    "summary(model, input_size)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the EuroSAT Data\n",
    "Use the function we defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded into memory\n",
      "using batch size 64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "dataloader_train, dataloader_val, dataloader_test = load_eurosat_data(custom_transform, batch_size=batch_size)\n",
    "print(\"data loaded into memory\")\n",
    "print(\"using batch size\", batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train & Test the Model\n",
    "Run the train, validate, and test loop we defined earlier. Each \".\" printed represents one batch run through the model, the loss calculated, and the gradient calculated, and the parameters updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started at 2025-12-15 13:52:02.548398\n",
      "Hyperparameters -- Batch Size: 64 Learning Rate: 0.0001 Epochs: 10\n",
      "Calculate Baseline Performance on Test Data Before Training...\n",
      "-------------------------------\n",
      "\n",
      "Test Error: \n",
      " Accuracy: 10.3%, Avg loss: 300.262173\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch started at 2025-12-15 13:52:20.968139\n",
      "..............................................................................................................................................................................................................................................................\n",
      "Validation Error: \n",
      " Accuracy: 78.9%, Avg loss: 0.676871\n",
      "Epoch training and validation lasted for 0:01:16.731134\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch started at 2025-12-15 13:53:37.699295\n",
      "..............................................................................................................................................................................................................................................................\n",
      "Validation Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.378905\n",
      "Epoch training and validation lasted for 0:01:18.017866\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch started at 2025-12-15 13:54:55.717187\n",
      "..............................................................................................................................................................................................................................................................\n",
      "Validation Error: \n",
      " Accuracy: 86.2%, Avg loss: 0.402683\n",
      "Epoch training and validation lasted for 0:01:18.393290\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch started at 2025-12-15 13:56:14.110514\n",
      "..............................................................................................................................................................................................................................................................\n",
      "Validation Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.709365\n",
      "Epoch training and validation lasted for 0:01:20.817974\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch started at 2025-12-15 13:57:34.928509\n",
      "..............................................................................................................................................................................................................................................................\n",
      "Validation Error: \n",
      " Accuracy: 85.4%, Avg loss: 0.453383\n",
      "Epoch training and validation lasted for 0:01:24.489037\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch started at 2025-12-15 13:58:59.417566\n",
      "..............................................................................................................................................................................................................................................................\n",
      "Validation Error: \n",
      " Accuracy: 92.0%, Avg loss: 0.245874\n",
      "Epoch training and validation lasted for 0:01:30.740294\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch started at 2025-12-15 14:00:30.157882\n",
      "..............................................................................................................................................................................................................................................................\n",
      "Validation Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.871029\n",
      "Epoch training and validation lasted for 0:01:33.333296\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch started at 2025-12-15 14:02:03.491198\n",
      "..............................................................................................................................................................................................................................................................\n",
      "Validation Error: \n",
      " Accuracy: 90.9%, Avg loss: 0.273177\n",
      "Epoch training and validation lasted for 0:01:28.156919\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch started at 2025-12-15 14:03:31.648144\n",
      "..............................................................................................................................................................................................................................................................\n",
      "Validation Error: \n",
      " Accuracy: 78.5%, Avg loss: 0.900815\n",
      "Epoch training and validation lasted for 0:01:23.413464\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch started at 2025-12-15 14:04:55.061629\n",
      "..............................................................................................................................................................................................................................................................\n",
      "Validation Error: \n",
      " Accuracy: 87.0%, Avg loss: 0.421406\n",
      "Epoch training and validation lasted for 0:01:22.553845\n",
      "Test Results\n",
      "-------------------------------\n",
      "\n",
      "Test Error: \n",
      " Accuracy: 87.4%, Avg loss: 0.400454\n",
      "Training completed in 0:14:33.930856 !\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "lr = 1e-4\n",
    "train_val_test(dataloader_train, dataloader_val, dataloader_test, model, lr=lr, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the Current Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/eurosat_resnet18_bs64_lr0001_epochs10.pth\n"
     ]
    }
   ],
   "source": [
    "save_model(model, \"eurosat_resnet18_bs64_lr0001_epochs10.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recreate Model with Pre-Trained Weights\n",
    "Some research has looked into using unlabeled data to \"pre-train\" models to be more prepared for downstream tasks. Here's we'll use a pretrained model from a group of researchers at the Technical University of Munich (TUM) who pretrained a model by aligning Sentinel-1 SAR imagery with Sentinel-2 multispectral imagery. Learn more about the dataset on their [Github](https://github.com/zhu-xlab/SSL4EO-S12), with the paper on [arXiv](https://arxiv.org/abs/2211.07044), or in the [torchgeo documentation](https://torchgeo.readthedocs.io/en/stable/_modules/torchgeo/models/resnet.html#ResNet18_Weights). TorchGeo makes it possible to preload our resnet with these pretrained weights in a single line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]          40,768\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "          Identity-7           [-1, 64, 56, 56]               0\n",
      "              ReLU-8           [-1, 64, 56, 56]               0\n",
      "          Identity-9           [-1, 64, 56, 56]               0\n",
      "           Conv2d-10           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-11           [-1, 64, 56, 56]             128\n",
      "             ReLU-12           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-13           [-1, 64, 56, 56]               0\n",
      "           Conv2d-14           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-15           [-1, 64, 56, 56]             128\n",
      "         Identity-16           [-1, 64, 56, 56]               0\n",
      "             ReLU-17           [-1, 64, 56, 56]               0\n",
      "         Identity-18           [-1, 64, 56, 56]               0\n",
      "           Conv2d-19           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-20           [-1, 64, 56, 56]             128\n",
      "             ReLU-21           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-24          [-1, 128, 28, 28]             256\n",
      "         Identity-25          [-1, 128, 28, 28]               0\n",
      "             ReLU-26          [-1, 128, 28, 28]               0\n",
      "         Identity-27          [-1, 128, 28, 28]               0\n",
      "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
      "           Conv2d-30          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-31          [-1, 128, 28, 28]             256\n",
      "             ReLU-32          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-33          [-1, 128, 28, 28]               0\n",
      "           Conv2d-34          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-35          [-1, 128, 28, 28]             256\n",
      "         Identity-36          [-1, 128, 28, 28]               0\n",
      "             ReLU-37          [-1, 128, 28, 28]               0\n",
      "         Identity-38          [-1, 128, 28, 28]               0\n",
      "           Conv2d-39          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-40          [-1, 128, 28, 28]             256\n",
      "             ReLU-41          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-44          [-1, 256, 14, 14]             512\n",
      "         Identity-45          [-1, 256, 14, 14]               0\n",
      "             ReLU-46          [-1, 256, 14, 14]               0\n",
      "         Identity-47          [-1, 256, 14, 14]               0\n",
      "           Conv2d-48          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-49          [-1, 256, 14, 14]             512\n",
      "           Conv2d-50          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-51          [-1, 256, 14, 14]             512\n",
      "             ReLU-52          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-53          [-1, 256, 14, 14]               0\n",
      "           Conv2d-54          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-55          [-1, 256, 14, 14]             512\n",
      "         Identity-56          [-1, 256, 14, 14]               0\n",
      "             ReLU-57          [-1, 256, 14, 14]               0\n",
      "         Identity-58          [-1, 256, 14, 14]               0\n",
      "           Conv2d-59          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-60          [-1, 256, 14, 14]             512\n",
      "             ReLU-61          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-62          [-1, 256, 14, 14]               0\n",
      "           Conv2d-63            [-1, 512, 7, 7]       1,179,648\n",
      "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
      "         Identity-65            [-1, 512, 7, 7]               0\n",
      "             ReLU-66            [-1, 512, 7, 7]               0\n",
      "         Identity-67            [-1, 512, 7, 7]               0\n",
      "           Conv2d-68            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-69            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-70            [-1, 512, 7, 7]         131,072\n",
      "      BatchNorm2d-71            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-72            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-73            [-1, 512, 7, 7]               0\n",
      "           Conv2d-74            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-75            [-1, 512, 7, 7]           1,024\n",
      "         Identity-76            [-1, 512, 7, 7]               0\n",
      "             ReLU-77            [-1, 512, 7, 7]               0\n",
      "         Identity-78            [-1, 512, 7, 7]               0\n",
      "           Conv2d-79            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-80            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-81            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-82            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-83            [-1, 512, 1, 1]               0\n",
      "          Flatten-84                  [-1, 512]               0\n",
      "SelectAdaptivePool2d-85                  [-1, 512]               0\n",
      "           Linear-86                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 11,213,002\n",
      "Trainable params: 11,213,002\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 2.49\n",
      "Forward/backward pass size (MB): 74.28\n",
      "Params size (MB): 42.77\n",
      "Estimated Total Size (MB): 119.54\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# this version already expects 13 input channels\n",
    "# set it to predict 10 classes\n",
    "# pull in the pretrained weights from SSL4EO-S12 MoCo \n",
    "pretrained_model = resnet18(weights=ResNet18_Weights.SENTINEL2_ALL_MOCO, in_chans=13, num_classes=10)\n",
    "\n",
    "# view the model again\n",
    "summary(pretrained_model, input_size)\n",
    "\n",
    "# move model to device\n",
    "model = pretrained_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train & Test the Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started at 2025-12-15 14:06:36.810891\n",
      "Hyperparameters -- Batch Size: 64 Learning Rate: 0.001 Epochs: 5\n",
      "Calculate Baseline Performance on Test Data Before Training...\n",
      "-------------------------------\n",
      "\n",
      "Test Error: \n",
      " Accuracy: 9.4%, Avg loss: 80.960480\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch started at 2025-12-15 14:06:53.926131\n",
      "..............................................................................................................................................................................................................................................................\n",
      "Validation Error: \n",
      " Accuracy: 83.4%, Avg loss: 0.582796\n",
      "Epoch training and validation lasted for 0:01:22.548241\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch started at 2025-12-15 14:08:16.474393\n",
      "..............................................................................................................................................................................................................................................................\n",
      "Validation Error: \n",
      " Accuracy: 95.8%, Avg loss: 0.135008\n",
      "Epoch training and validation lasted for 0:01:19.724094\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch started at 2025-12-15 14:09:36.198506\n",
      "..............................................................................................................................................................................................................................................................\n",
      "Validation Error: \n",
      " Accuracy: 92.6%, Avg loss: 0.245330\n",
      "Epoch training and validation lasted for 0:01:20.216896\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch started at 2025-12-15 14:10:56.415422\n",
      "..............................................................................................................................................................................................................................................................\n",
      "Validation Error: \n",
      " Accuracy: 95.1%, Avg loss: 0.157644\n",
      "Epoch training and validation lasted for 0:01:21.124914\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch started at 2025-12-15 14:12:17.540356\n",
      "..............................................................................................................................................................................................................................................................\n",
      "Validation Error: \n",
      " Accuracy: 92.7%, Avg loss: 0.228106\n",
      "Epoch training and validation lasted for 0:01:25.859286\n",
      "Test Results\n",
      "-------------------------------\n",
      "\n",
      "Test Error: \n",
      " Accuracy: 93.4%, Avg loss: 0.232597\n",
      "Training completed in 0:07:25.546174 !\n"
     ]
    }
   ],
   "source": [
    "train_val_test(dataloader_train, dataloader_val, dataloader_test, pretrained_model, lr=1e-3, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Down the Fine-Tuned Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/eurosat_resnet18_pretrained_bs64_lr001_epochs5.pth\n"
     ]
    }
   ],
   "source": [
    "save_model(model, \"eurosat_resnet18_pretrained_bs64_lr001_epochs5.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a Given Model\n",
    "If you have a model saved that you would like to train further, use the code below to load the weights so that you can evaluate it or continue training. If you continue training, I suggest using the same hyperparameters that you used to train that model in the first place, just run it for more epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_model(experiment_name):\n",
    "    # create an empty res net with the right io dimensions\n",
    "    model = resnet18(weights=None, in_chans=13, num_classes=10).to(device)\n",
    "    # create the path to the model weights\n",
    "    model_weights_path = os.path.join(\"models\", experiment_name)\n",
    "    # load the model weights from the path\n",
    "    model.load_state_dict(torch.load(model_weights_path, weights_only=True))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded and ready\n"
     ]
    }
   ],
   "source": [
    "experiment_name = \"eurosat_resnet18_bs64_lr0001_epochs10.pth\"\n",
    "model = load_model(experiment_name)\n",
    "print(\"model loaded and ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "1. **Maximize Training Efficiency**: Before trying to train your best model for many epochs, try out different batch sizes (ie. 16, 32, 64, 128, 256, 512, etc), number of workers (ie. 0,1,2,4,8), and which GPU (ie GPU P100, T4x2) for training just 1 epoch. See which configuration is able to train an epoch the fastest. You should be able to get it under two minutes for a single epoch. Take note of CPU and GPU usage in the top right dashboard as training happens. Record what the most efficient configuration is and use that for the next part. Write down what you're trying and what the results are.\n",
    "\n",
    "2. **Maximize Accuracy**: Now mess with the hyperparameters to see what combination leads to the highest accuracy. Namely, experiment with the number of epochs, the learning rate, and if you're feeling curious, potentially even the optimizer and the loss function to achieve the best accuracy. Report your results in a table like this one. Compare your results to the results reported in the original [EuroSAT paper from 2017](https://arxiv.org/abs/1709.00029) and the [SSL4EO-S12 paper from 2022](https://arxiv.org/abs/2211.07044).\n",
    "\n",
    "|   Dataset |   Img Tx         | Model         |LR   | BS | Epochs | Accuracy  |\n",
    "|-----------|------------------|---------------|-----|----|--------|-----------|\n",
    "|   EuroSAT | Resize           | ResNet18      |0.001| 64 | 1      | 74.8%     |\n",
    "|   EuroSAT | Resize           | ResNet18      |0.001| 128| 5      | 87.6%     |\n",
    "|   EuroSAT | Resize           | ResNet18 MoCo |0.001| 64 | 10     | 97.6%     |\n",
    "|   EuroSAT | Resize           | ResNet18 MoCo |0.001| 256| 20     | 74.4%     |\n",
    "|   EuroSAT | Resize           | ResNet50      |0.001| 64 | 30     | 86.2%     |\n",
    "|   EuroSAT | Resize           | ResNet50      |0.001| 512| 50     | 91.8%     |\n",
    "|   EuroSAT | Resize & Flip VH | ResNet50 MoCo |0.001| 32 | 5      | 60.4%     |\n",
    "|   EuroSAT | Resize & Flip VH | ResNet50 MoCo |0.001| 64 | 10     | 95.7%     |\n",
    "\n",
    "3. **Reflection Question**: What applications could a highly accurate land cover classification model be used for that relate to the [UN's 17 Sustainable Development Goals](https://sdgs.un.org/goals)? Write out your answer.\n",
    "\n",
    "## Bonus Challenge #1: Plotting Loss per Epoch\n",
    "A common practice in ML is to report your loss over time as you train and validate your model with each epoch. For this challenge, implement code that records the training and validation loss after each epoch and then plot it. \n",
    "\n",
    "## Bonus Challenge #2: F1 Score\n",
    "The [F1 score](https://www.geeksforgeeks.org/f1-score-in-machine-learning/) is a more nuanced metric than accuracy that focusses on precision and recall. Implement the reporting of the F1 score of your model alongside the accuracy when evaluating the model on validation or test data.\n",
    "\n",
    "## Bonus Challenge #3: Adding Data Augmentation\n",
    "The code below adds in some data augmentations before running the data through the model. This is a technique that is often used to get the most out of your training data and ensure that the representations learned by the model are as general as possible. For this challenge, use the code below as a guide to add data augmentations to the training pipeline. Feel free to use whatever data augmentations you'd like, just provide a justification as to why you used those. Compare how this affects your accuracy on the validation and test data. Reflect on why you are seeing the results you are. Below is a code chunk that should help you as a starting point:\n",
    "```{python}\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Randomly flip the image horizontally\n",
    "    transforms.RandomVerticalFlip(p=0.5),  # Randomly flip the image vertically\n",
    "    transforms.Resize((224, 224))  # Resizes the images to 224x224\n",
    "])\n",
    "\n",
    "custom_transform = CustomTransform(transform)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Notebook to HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook eurosat-torchgeo.ipynb to html\n",
      "[NbConvertApp] WARNING | Alternative text is missing on 1 image(s).\n",
      "[NbConvertApp] Writing 395438 bytes to eurosat-torchgeo.html\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# export to HTML for webpage\n",
    "import os\n",
    "os.system('jupyter nbconvert --to html eurosat-torchgeo.ipynb --HTMLExporter.theme=dark')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "tg2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
