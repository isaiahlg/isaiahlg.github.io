{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Wealth & Poverty with MOSAIKS\n",
    "To predict the relative wealth of the people in a given area that was not surveyed, we will start with a geospatial foundation model called MOSAIKS. MOSAIKS stands for \"MULTI-TASK OBSERVATION USING SATELLITE IMAGERY & KITCHEN SINKS\" and uses random convolutional features to extract n features from a satellite image, each corresponding to a given random filter. \n",
    "\n",
    "* Nature Paper: https://www.nature.com/articles/s41467-021-24638-z\n",
    "* Website with API: https://api.mosaiks.org/portal/index/\n",
    "* GitHub Repo: https://github.com/Global-Policy-Lab/mosaiks-paper\n",
    "\n",
    "![mosaiks](https://images.squarespace-cdn.com/content/v1/64090ac2649ae84371ef65cc/0ee082a3-9b0c-419a-aea1-c3bb8a642d19/MOSAIKS_HDI_Highres.jpg)\n",
    "\n",
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# install any libraries that are missing\n",
    "!pip install basemap --quiet\n",
    "print(\"libraries installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# for fetching data\n",
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# for data processing\n",
    "import zipfile\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "\n",
    "# for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "# for regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# set the random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"imported\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# configure parameters\n",
    "decimal_place = 0 # spatial resolution of the mosaiks embeddings in degrees of lat/long\n",
    "mosaiks_data_url = f\"https://api.mosaiks.org/portal/download_grid_file/coarsened_global_dense_grid_decimal_place={decimal_place}_GHS_pop_weight=True.zip\"\n",
    "mosaiks_data_path = f\"./mosaiks{decimal_place}/\"\n",
    "zip_file_path = f\"{mosaiks_data_path}global_{decimal_place}.zip\"\n",
    "mosaiks_embeddings_file_path = f\"{mosaiks_data_path}coarsened_global_dense_grid_decimal_place={decimal_place}_GHS_pop_weight=True.csv\"\n",
    "labels_filename = \"/kaggle/input/dhs_final_labels.csv\"\n",
    "labels_centered_filename = \"./dhs_final_labels_centered.csv\"\n",
    "labels_merged_filename = \"./dhs_final_labels_centered_with_mosaiks.csv\"\n",
    "\n",
    "os.makedirs(mosaiks_data_path, exist_ok=True)\n",
    "print(\"configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Data from MOSAIKS Website\n",
    "They have an API to extract embeddings for a given location, but it's down right now. Instead, we'll download a coarse resolution version with an embedding on a grid every 0.25 degress on Earth over land. The zipped file is about 5GB and takes a few minutes to download. We'll then unzip it so we can find the embeddings closest to our labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cookie for authentication, scraped from a web browser session, please do not share\n",
    "COOKIE = {\n",
    "    \"csrftoken\": \"I9x2jvGGE4se3MBa9moavDtC9o8YEgaA4Rup5ijhHJjCTRn0qRpHGJW06XG0SooG\",\n",
    "    \"sessionid\": \"y44nlmh7rjrqvvxj902jc8pmw918m1p7\",\n",
    "}\n",
    "\n",
    "# Function to download a file with progress bar\n",
    "def download_file(url, filename):\n",
    "    response = requests.get(url, cookies=COOKIE, stream=True, verify=False)\n",
    "    # handle request\n",
    "    if response.status_code == 200:\n",
    "        total_size = int(response.headers.get(\"content-length\", 0))\n",
    "        with open(filename, \"wb\") as f, tqdm(\n",
    "            desc=os.path.basename(filename),\n",
    "            total=total_size,\n",
    "            unit=\"B\",\n",
    "            unit_scale=True,\n",
    "            unit_divisor=1024,\n",
    "        ) as bar:\n",
    "            for chunk in response.iter_content(1024):\n",
    "                f.write(chunk)\n",
    "                bar.update(len(chunk))\n",
    "        print(f\"‚úÖ Downloaded: {filename}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Failed to download {url} (Status {response.status_code})\")\n",
    "\n",
    "# Download each file\n",
    "# Base URL for file downloads\n",
    "\n",
    "print(f\"üì• Downloading {mosaiks_data_url}...\")\n",
    "download_file(mosaiks_data_url, zip_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# unzip all downloaded files\n",
    "for zip_file in glob.glob(os.path.join(mosaiks_data_path, \"*.zip\")):\n",
    "    # Open the zip file\n",
    "    with zipfile.ZipFile(zip_file, \"r\") as z:\n",
    "        print(f\"Extracting {zip_file} to {mosaiks_data_path}...\")\n",
    "        # Extract all the contents into the extraction directory\n",
    "        z.extractall(mosaiks_data_path)\n",
    "print('done unzipping!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Wealth Data Labels to Merge with MOSAIKS Embeddings\n",
    "Now we need to match up our locations of our wealth data with the nearest MOSAIKS locations on the global grid of embeddings. We'll do that with some decimal rounding tricks. Some labels will share the same nearest neighbor, so we'll just pick the first one that has a non-null label for `asset_index`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load labels dataset\n",
    "print(\"Reading in data labels...\")\n",
    "df = pd.read_csv(labels_filename)\n",
    "\n",
    "\n",
    "# drop rows that have a NaN for asset_index\n",
    "print(\"Dropping any rows that don't have a value for asset index...\")\n",
    "before_len = len(df)\n",
    "df = df.dropna(subset=[\"asset_index\"])\n",
    "print(f\"Started with {before_len} rows, now have {len(df)} rows\")\n",
    "\n",
    "# Assign coordinates to nearest tile centroid\n",
    "print(\"Finding nearest MOSAIKS embedding locations...\")\n",
    "# for 1 degree\n",
    "if decimal_place == 0:\n",
    "    df[\"lon2\"] = round(round(df[\"lon\"] + 0.5, 0) - 0.5, 1)\n",
    "    df[\"lat2\"] = round(round(df[\"lat\"] + 0.5, 0) - 0.5, 1)\n",
    "# for 1/4 degree\n",
    "if decimal_place == 0.25:\n",
    "    df[\"lon2\"] = round((round(df[\"lon\"]*4 + 0.5, 0) - 0.5)/4, 3)\n",
    "    df[\"lat2\"] = round((round(df[\"lat\"]*4 + 0.5, 0) - 0.5)/4, 3)\n",
    "\n",
    "# keep just one row for every unique Lat, Lon combination\n",
    "print(\"Dropping labels that share the same embedding location...\")\n",
    "before_len = len(df)\n",
    "df = df.drop_duplicates(subset=[\"lat2\", \"lon2\"], keep=\"first\")\n",
    "print(f\"Started with {before_len} rows, now have {len(df)} rows\")\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "print(\"Converting to a geodataframe and exporting to csv...\")\n",
    "gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df[\"lon2\"], df[\"lat2\"]), crs=\"EPSG:4326\")\n",
    "gdf.to_csv(labels_centered_filename, index=False)\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize the data. It should appear in a grid pattern at 0.25 degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(12, 6))\n",
    "\n",
    "m = Basemap(projection='cyl', resolution='c', ax=ax)\n",
    "m.drawcoastlines()\n",
    "ax.scatter(df[\"lon2\"], df[\"lat2\"], c=df[\"asset_index\"], s=1)\n",
    "ax.set_title('Mean Asset Index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Embeddings with Labels\n",
    "Now that the data points have been moved to the centroids of the clusters, we can merge the embeddings with the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "mosaiks = pd.read_csv(mosaiks_embeddings_file_path)\n",
    "mosaiks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(labels_centered_filename)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# merge the two dataframes on the Lat, Lon columns from the labels and the lat, lon columns from the mosaiks\n",
    "mosaiks = mosaiks.rename(columns={\"lat\": \"lat2\", \"lon\": \"lon2\"})\n",
    "df_merged = df.merge(mosaiks, on=[\"lat2\", \"lon2\"], how=\"left\")\n",
    "\n",
    "# save this as a csv\n",
    "df_merged.to_csv(labels_merged_filename, index=False)\n",
    "\n",
    "# inspect the data\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep Data for Predicting Wealth Using Regression\n",
    "1. Drop any rows with missing label values\n",
    "2. Split into test / validation / train datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# read in the data\n",
    "df = pd.read_csv(labels_merged_filename)\n",
    "\n",
    "# keep just the columns we need\n",
    "df = df[[col for col in df.columns if col.startswith(\"X_\")] + [\"asset_index\"]]\n",
    "# check for NaNs\n",
    "print(\"Number of NaNs in df: \", df.isna().sum())\n",
    "# drop rows with NaNs\n",
    "df = df.dropna()\n",
    "# check the shape again\n",
    "print(\"Shape of df: \", df.shape)\n",
    "\n",
    "# split the data into train and test sets\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "# split the test set into val and test sets\n",
    "df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=42)\n",
    "# check the shapes of the data\n",
    "print(\"Shape of df_train: \", df_train.shape)\n",
    "print(\"Shape of df_val: \", df_val.shape)\n",
    "print(\"Shape of df_test: \", df_test.shape)\n",
    "\n",
    "\n",
    "# separate the data into X and y\n",
    "y_train = df_train[\"asset_index\"]\n",
    "y_val = df_val[\"asset_index\"]\n",
    "y_test = df_test[\"asset_index\"]\n",
    "\n",
    "# x is all the data in columns with names formatted like \"X_1\"\n",
    "X_train = df_train[[col for col in df.columns if col.startswith(\"X_\")]]\n",
    "X_val = df_val[[col for col in df.columns if col.startswith(\"X_\")]]\n",
    "X_test = df_test[[col for col in df.columns if col.startswith(\"X_\")]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Linear Classifier using Ridge Regression\n",
    "Now that we have our embeddings and our labels, we can run a linear classifier on top of the embeddings to see how well each model does at predicting poverty and wealth. We will use Ridge regression to do this, since it is a linear model that can handle high dimensional data well. It uses L2 regularization to prevent overfitting, penalizing large weights. You can learn more about it in [this Medium post](https://medium.com/@msoczi/ridge-regression-step-by-step-introduction-with-example-0d22dddb7d54) or in this tutorial from [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2016/01/ridge-lasso-regression-python-complete-tutorial/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define alpha values to loop over\n",
    "alpha_values = [10**i for i in range(-5, 4)]  # Corrected alpha values from 0.001 to 1000\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, alpha in enumerate(alpha_values):\n",
    "    ridge_model = Ridge(alpha=alpha)\n",
    "    ridge_model.fit(X_train, y_train)\n",
    "    y_pred = ridge_model.predict(X_val)\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "    \n",
    "    # Create scatter plot using Seaborn\n",
    "    sns.scatterplot(x=y_val, y=y_pred, ax=axes[i], s=10)\n",
    "    axes[i].plot([-4, 4], [-4, 4], \"r--\")  # 45-degree line\n",
    "    \n",
    "    # Fit a regression line with confidence interval\n",
    "    sns.regplot(x=y_val, y=y_pred, ax=axes[i], scatter=False, ci=95, line_kws={\"color\": \"blue\"})\n",
    "    \n",
    "    axes[i].set_title(f\"Validation Performance, Alpha={alpha}, R¬≤={r2:.2f}\")\n",
    "    axes[i].set_xlabel(\"Asset Index (True)\")\n",
    "    axes[i].set_ylabel(\"Asset Index (Predicted)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the top performing value for alpha on the validation dataset is 0.01. For a final test of the model, let's run this regression model trained on the test data, tuned on the validation, and see how well it performs on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Select the best alpha value from the validation data\n",
    "alpha = 0.01\n",
    "\n",
    "# Train Ridge regression model on training data\n",
    "ridge_model = Ridge(alpha=alpha)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "coefficients = ridge_model.coef_\n",
    "\n",
    "# Use coefficients from training for inference on test data\n",
    "y_pred = X_test @ coefficients + ridge_model.intercept_\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Create scatter plot using Seaborn\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(x=y_test, y=y_pred, s=10)\n",
    "plt.plot([-4, 4], [-4, 4], \"r--\")  # 45-degree line\n",
    "\n",
    "# Fit a regression line with confidence interval\n",
    "sns.regplot(x=y_test, y=y_pred, scatter=False, ci=95, line_kws={\"color\": \"blue\"})\n",
    "\n",
    "plt.title(f\"Test Data Peformance, Alpha={alpha}, R¬≤={r2:.2f}\")\n",
    "plt.xlabel(\"Asset Index (True)\")\n",
    "plt.ylabel(\"Asset Index (Predicted)\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Wealth Prediction\n",
    "Now that we have a model that can predict wealth, let's use it to predict wealth across the entire globe. We will use the MOSAIKS API to get embeddings for the entire world, and then use our model to predict wealth for each location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Retrain Ridge regression model on training data\n",
    "print(\"Retraining ridge model on training data...\")\n",
    "alpha = 0.01\n",
    "ridge_model = Ridge(alpha=alpha)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "coefficients = ridge_model.coef_\n",
    "\n",
    "print(\"reading in the global dataset...\")\n",
    "# read in the global dataset of mosaiks embeddings\n",
    "mosaiks = pd.read_csv(mosaiks_embeddings_file_path)\n",
    "print(\"read in dataset\")\n",
    "\n",
    "lat_global = mosaiks[\"lat\"]\n",
    "lon_global = mosaiks[\"lon\"]\n",
    "# pull out the embeddings from the dataframe, columns that begin with \"X_\"\n",
    "X_global = mosaiks.drop(columns=[\"lat\", \"lon\", \"continent\"])\n",
    "print(\"extracted just embeddings\")\n",
    "# Select the best alpha value from the validation data\n",
    "alpha = 0.01\n",
    "# Train Ridge regression model on training data\n",
    "coefficients = ridge_model.coef_\n",
    "print(\"fitted ridge model to training data\")\n",
    "\n",
    "# Use coefficients from training for inference on test data\n",
    "print(\"running inference on the global data...\")\n",
    "y_pred_global = X_global @ coefficients + ridge_model.intercept_\n",
    "print(\"inference complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the global predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Merge predictions with lat/lon, name prediction column \"pred\"\n",
    "df_global = pd.concat([lat_global, lon_global, pd.Series(y_pred_global)], axis=1)\n",
    "df_global.columns = [\"lat\", \"lon\", \"pred\"]\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "gdf_global = gpd.GeoDataFrame(df_global, geometry=gpd.points_from_xy(df_global[\"lon\"], df_global[\"lat\"]), crs=\"EPSG:4326\")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(12, 6))\n",
    "\n",
    "m = Basemap(projection='cyl', resolution='c', ax=ax)\n",
    "m.drawcoastlines()\n",
    "\n",
    "sc = ax.scatter(gdf_global[\"lon\"], gdf_global[\"lat\"], c=gdf_global[\"pred\"], s=2, cmap='viridis')\n",
    "cb = fig.colorbar(sc, ax=ax, orientation='vertical', label='Predicted Mean Asset Index')\n",
    "\n",
    "ax.set_title('Mean Asset Index Global Prediction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "1. Answer the following questions based on the regression figures above:\n",
    "    - What's going on with the small alpha values? Why are most data points squished up towards the top? Why is there such a strong outlier?\n",
    "    - What do you think is going on with the large alpha values? Why does the R-squared value decrease again? \n",
    "    - The red line shows a 45-degree line of a theoretical perfect fit. The blue line shows an approximation of the actual fit. Why is the blue line consistently flatter than the red line? What does this mean for our model? How could we improve that?\n",
    "\n",
    "2. What do you make of our global inference map? Does it match up with your intuition of wealthy and poor areas of the world? Do you think this is a valid approach for making a global inference? What pitfalls might this approach be prone to?\n",
    "\n",
    "3. Pick another variable from the SustainBench poverty dataset and repeat the analysis above. Do you see an even stronger correlation with MOSAIKS embeddings? A weaker one? Why do you think that is?\n",
    "\n",
    "\n",
    "## Bonus Assignment 1: Redo Analysis Higher Resolution Embeddings\n",
    "Instead of using the low-resolution embeddings aggregated to every 0.25 degrees, we have the original embeddings at every 0.01 degrees for each of the points in the SustainBench dataset in Kaggle [here](https://www.kaggle.com/datasets/isaiahlg/mosaiks-embeddings-for-sustainbench-wealth-mapping/). Thanks to [Luke Sherman](https://www.globalpolicy.science/luke-sherman) from Stanford for pulling them for us even though the MOSAIKS API was down. How does the spatial resolution of the DHS clusters and the dense MOSAIKS embeddings compare? Repeat the analysis using the dense embeddings. How does the accuracy compare with the coarse resolution embeddings?\n",
    "\n",
    "## Bonus Assignment 2: Geospatial Errors\n",
    "Associate the errors in the model above with the original geospatial datapoints in the SustainBench dataset and plot them on a map. Do you see any patterns? Are there any areas of the map that perform worse than others? If so, what do you think you could do to reduce the geographic bias of the model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Notebook to HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This application is used to convert notebook files (*.ipynb)\n",
      "        to various other formats.\n",
      "\n",
      "        WARNING: THE COMMANDLINE INTERFACE MAY CHANGE IN FUTURE RELEASES.\n",
      "\n",
      "Options\n",
      "=======\n",
      "The options below are convenience aliases to configurable class-options,\n",
      "as listed in the \"Equivalent to\" description-line of the aliases.\n",
      "To see all configurable class-options for some <cmd>, use:\n",
      "    <cmd> --help-all\n",
      "\n",
      "--debug\n",
      "    set log level to logging.DEBUG (maximize logging output)\n",
      "    Equivalent to: [--Application.log_level=10]\n",
      "--show-config\n",
      "    Show the application's configuration (human-readable format)\n",
      "    Equivalent to: [--Application.show_config=True]\n",
      "--show-config-json\n",
      "    Show the application's configuration (json format)\n",
      "    Equivalent to: [--Application.show_config_json=True]\n",
      "--generate-config\n",
      "    generate default config file\n",
      "    Equivalent to: [--JupyterApp.generate_config=True]\n",
      "-y\n",
      "    Answer yes to any questions instead of prompting.\n",
      "    Equivalent to: [--JupyterApp.answer_yes=True]\n",
      "--execute\n",
      "    Execute the notebook prior to export.\n",
      "    Equivalent to: [--ExecutePreprocessor.enabled=True]\n",
      "--allow-errors\n",
      "    Continue notebook execution even if one of the cells throws an error and include the error message in the cell output (the default behaviour is to abort conversion). This flag is only relevant if '--execute' was specified, too.\n",
      "    Equivalent to: [--ExecutePreprocessor.allow_errors=True]\n",
      "--stdin\n",
      "    read a single notebook file from stdin. Write the resulting notebook with default basename 'notebook.*'\n",
      "    Equivalent to: [--NbConvertApp.from_stdin=True]\n",
      "--stdout\n",
      "    Write notebook output to stdout instead of files.\n",
      "    Equivalent to: [--NbConvertApp.writer_class=StdoutWriter]\n",
      "--inplace\n",
      "    Run nbconvert in place, overwriting the existing notebook (only\n",
      "            relevant when converting to notebook format)\n",
      "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory=]\n",
      "--clear-output\n",
      "    Clear output of current file and save in place,\n",
      "            overwriting the existing notebook.\n",
      "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory= --ClearOutputPreprocessor.enabled=True]\n",
      "--coalesce-streams\n",
      "    Coalesce consecutive stdout and stderr outputs into one stream (within each cell).\n",
      "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory= --CoalesceStreamsPreprocessor.enabled=True]\n",
      "--no-prompt\n",
      "    Exclude input and output prompts from converted document.\n",
      "    Equivalent to: [--TemplateExporter.exclude_input_prompt=True --TemplateExporter.exclude_output_prompt=True]\n",
      "--no-input\n",
      "    Exclude input cells and output prompts from converted document.\n",
      "            This mode is ideal for generating code-free reports.\n",
      "    Equivalent to: [--TemplateExporter.exclude_output_prompt=True --TemplateExporter.exclude_input=True --TemplateExporter.exclude_input_prompt=True]\n",
      "--allow-chromium-download\n",
      "    Whether to allow downloading chromium if no suitable version is found on the system.\n",
      "    Equivalent to: [--WebPDFExporter.allow_chromium_download=True]\n",
      "--disable-chromium-sandbox\n",
      "    Disable chromium security sandbox when converting to PDF..\n",
      "    Equivalent to: [--WebPDFExporter.disable_sandbox=True]\n",
      "--show-input\n",
      "    Shows code input. This flag is only useful for dejavu users.\n",
      "    Equivalent to: [--TemplateExporter.exclude_input=False]\n",
      "--embed-images\n",
      "    Embed the images as base64 dataurls in the output. This flag is only useful for the HTML/WebPDF/Slides exports.\n",
      "    Equivalent to: [--HTMLExporter.embed_images=True]\n",
      "--sanitize-html\n",
      "    Whether the HTML in Markdown cells and cell outputs should be sanitized..\n",
      "    Equivalent to: [--HTMLExporter.sanitize_html=True]\n",
      "--log-level=<Enum>\n",
      "    Set the log level by value or name.\n",
      "    Choices: any of [0, 10, 20, 30, 40, 50, 'DEBUG', 'INFO', 'WARN', 'ERROR', 'CRITICAL']\n",
      "    Default: 30\n",
      "    Equivalent to: [--Application.log_level]\n",
      "--config=<Unicode>\n",
      "    Full path of a config file.\n",
      "    Default: ''\n",
      "    Equivalent to: [--JupyterApp.config_file]\n",
      "--to=<Unicode>\n",
      "    The export format to be used, either one of the built-in formats\n",
      "            ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'qtpdf', 'qtpng', 'rst', 'script', 'slides', 'webpdf']\n",
      "            or a dotted object name that represents the import path for an\n",
      "            ``Exporter`` class\n",
      "    Default: ''\n",
      "    Equivalent to: [--NbConvertApp.export_format]\n",
      "--template=<Unicode>\n",
      "    Name of the template to use\n",
      "    Default: ''\n",
      "    Equivalent to: [--TemplateExporter.template_name]\n",
      "--template-file=<Unicode>\n",
      "    Name of the template file to use\n",
      "    Default: None\n",
      "    Equivalent to: [--TemplateExporter.template_file]\n",
      "--theme=<Unicode>\n",
      "    Template specific theme(e.g. the name of a JupyterLab CSS theme distributed\n",
      "    as prebuilt extension for the lab template)\n",
      "    Default: 'light'\n",
      "    Equivalent to: [--HTMLExporter.theme]\n",
      "--sanitize_html=<Bool>\n",
      "    Whether the HTML in Markdown cells and cell outputs should be sanitized.This\n",
      "    should be set to True by nbviewer or similar tools.\n",
      "    Default: False\n",
      "    Equivalent to: [--HTMLExporter.sanitize_html]\n",
      "--writer=<DottedObjectName>\n",
      "    Writer class used to write the\n",
      "                                        results of the conversion\n",
      "    Default: 'FilesWriter'\n",
      "    Equivalent to: [--NbConvertApp.writer_class]\n",
      "--post=<DottedOrNone>\n",
      "    PostProcessor class used to write the\n",
      "                                        results of the conversion\n",
      "    Default: ''\n",
      "    Equivalent to: [--NbConvertApp.postprocessor_class]\n",
      "--output=<Unicode>\n",
      "    Overwrite base name use for output files.\n",
      "                Supports pattern replacements '{notebook_name}'.\n",
      "    Default: '{notebook_name}'\n",
      "    Equivalent to: [--NbConvertApp.output_base]\n",
      "--output-dir=<Unicode>\n",
      "    Directory to write output(s) to. Defaults\n",
      "                                  to output to the directory of each notebook. To recover\n",
      "                                  previous default behaviour (outputting to the current\n",
      "                                  working directory) use . as the flag value.\n",
      "    Default: ''\n",
      "    Equivalent to: [--FilesWriter.build_directory]\n",
      "--reveal-prefix=<Unicode>\n",
      "    The URL prefix for reveal.js (version 3.x).\n",
      "            This defaults to the reveal CDN, but can be any url pointing to a copy\n",
      "            of reveal.js.\n",
      "            For speaker notes to work, this must be a relative path to a local\n",
      "            copy of reveal.js: e.g., \"reveal.js\".\n",
      "            If a relative path is given, it must be a subdirectory of the\n",
      "            current directory (from which the server is run).\n",
      "            See the usage documentation\n",
      "            (https://nbconvert.readthedocs.io/en/latest/usage.html#reveal-js-html-slideshow)\n",
      "            for more details.\n",
      "    Default: ''\n",
      "    Equivalent to: [--SlidesExporter.reveal_url_prefix]\n",
      "--nbformat=<Enum>\n",
      "    The nbformat version to write.\n",
      "            Use this to downgrade notebooks.\n",
      "    Choices: any of [1, 2, 3, 4]\n",
      "    Default: 4\n",
      "    Equivalent to: [--NotebookExporter.nbformat_version]\n",
      "\n",
      "Examples\n",
      "--------\n",
      "\n",
      "    The simplest way to use nbconvert is\n",
      "\n",
      "            > jupyter nbconvert mynotebook.ipynb --to html\n",
      "\n",
      "            Options include ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'qtpdf', 'qtpng', 'rst', 'script', 'slides', 'webpdf'].\n",
      "\n",
      "            > jupyter nbconvert --to latex mynotebook.ipynb\n",
      "\n",
      "            Both HTML and LaTeX support multiple output templates. LaTeX includes\n",
      "            'base', 'article' and 'report'.  HTML includes 'basic', 'lab' and\n",
      "            'classic'. You can specify the flavor of the format used.\n",
      "\n",
      "            > jupyter nbconvert --to html --template lab mynotebook.ipynb\n",
      "\n",
      "            You can also pipe the output to stdout, rather than a file\n",
      "\n",
      "            > jupyter nbconvert mynotebook.ipynb --stdout\n",
      "\n",
      "            PDF is generated via latex\n",
      "\n",
      "            > jupyter nbconvert mynotebook.ipynb --to pdf\n",
      "\n",
      "            You can get (and serve) a Reveal.js-powered slideshow\n",
      "\n",
      "            > jupyter nbconvert myslides.ipynb --to slides --post serve\n",
      "\n",
      "            Multiple notebooks can be given at the command line in a couple of\n",
      "            different ways:\n",
      "\n",
      "            > jupyter nbconvert notebook*.ipynb\n",
      "            > jupyter nbconvert notebook1.ipynb notebook2.ipynb\n",
      "\n",
      "            or you can specify the notebooks list in a config file, containing::\n",
      "\n",
      "                c.NbConvertApp.notebooks = [\"my_notebook.ipynb\"]\n",
      "\n",
      "            > jupyter nbconvert --config mycfg.py\n",
      "\n",
      "To see all available configurables, use `--help-all`.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "65280"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# export to HTML for webpage\n",
    "import os\n",
    "os.system('jupyter nbconvert --to html pt2-mosaiks.ipynb --HTMLExporter.theme=dark')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6635435,
     "sourceId": 10707414,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30887,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "pd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
