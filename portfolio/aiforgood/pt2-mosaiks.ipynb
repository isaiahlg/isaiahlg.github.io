{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Wealth & Poverty with MOSAIKS\n",
    "To predict the relative wealth of the people in a given area that was not surveyed, we will start with a geospatial foundation model called MOSAIKS. MOSAIKS stands for \"MULTI-TASK OBSERVATION USING SATELLITE IMAGERY & KITCHEN SINKS\" and uses random convolutional features to extract n features from a satellite image, each corresponding to a given random filter. \n",
    "\n",
    "* Nature Paper: https://www.nature.com/articles/s41467-021-24638-z\n",
    "* Website with API: https://api.mosaiks.org/portal/index/\n",
    "* GitHub Repo: https://github.com/Global-Policy-Lab/mosaiks-paper\n",
    "\n",
    "![mosaiks](https://images.squarespace-cdn.com/content/v1/64090ac2649ae84371ef65cc/0ee082a3-9b0c-419a-aea1-c3bb8a642d19/MOSAIKS_HDI_Highres.jpg)\n",
    "\n",
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# install any libraries that are missing\n",
    "!pip install basemap --quiet\n",
    "print(\"libraries installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# for fetching data\n",
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# for data processing\n",
    "import zipfile\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "\n",
    "# for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "# for regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# set the random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"imported\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# configure parameters\n",
    "decimal_place = 0 # spatial resolution of the mosaiks embeddings in degrees of lat/long\n",
    "mosaiks_data_url = f\"https://api.mosaiks.org/portal/download_grid_file/coarsened_global_dense_grid_decimal_place={decimal_place}_GHS_pop_weight=True.zip\"\n",
    "mosaiks_data_path = f\"./mosaiks{decimal_place}/\"\n",
    "zip_file_path = f\"{mosaiks_data_path}global_{decimal_place}.zip\"\n",
    "mosaiks_embeddings_file_path = f\"{mosaiks_data_path}coarsened_global_dense_grid_decimal_place={decimal_place}_GHS_pop_weight=True.csv\"\n",
    "labels_filename = \"/kaggle/input/dhs_final_labels.csv\"\n",
    "labels_centered_filename = \"./dhs_final_labels_centered.csv\"\n",
    "labels_merged_filename = \"./dhs_final_labels_centered_with_mosaiks.csv\"\n",
    "\n",
    "os.makedirs(mosaiks_data_path, exist_ok=True)\n",
    "print(\"configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Data from MOSAIKS Website\n",
    "They have an API to extract embeddings for a given location, but it's down right now. Instead, we'll download a coarse resolution version with an embedding on a grid every 0.25 degress on Earth over land. The zipped file is about 5GB and takes a few minutes to download. We'll then unzip it so we can find the embeddings closest to our labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cookie for authentication, scraped from a web browser session, please do not share\n",
    "COOKIE = {\n",
    "    \"csrftoken\": \"I9x2jvGGE4se3MBa9moavDtC9o8YEgaA4Rup5ijhHJjCTRn0qRpHGJW06XG0SooG\",\n",
    "    \"sessionid\": \"y44nlmh7rjrqvvxj902jc8pmw918m1p7\",\n",
    "}\n",
    "\n",
    "# Function to download a file with progress bar\n",
    "def download_file(url, filename):\n",
    "    response = requests.get(url, cookies=COOKIE, stream=True, verify=False)\n",
    "    # handle request\n",
    "    if response.status_code == 200:\n",
    "        total_size = int(response.headers.get(\"content-length\", 0))\n",
    "        with open(filename, \"wb\") as f, tqdm(\n",
    "            desc=os.path.basename(filename),\n",
    "            total=total_size,\n",
    "            unit=\"B\",\n",
    "            unit_scale=True,\n",
    "            unit_divisor=1024,\n",
    "        ) as bar:\n",
    "            for chunk in response.iter_content(1024):\n",
    "                f.write(chunk)\n",
    "                bar.update(len(chunk))\n",
    "        print(f\"‚úÖ Downloaded: {filename}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Failed to download {url} (Status {response.status_code})\")\n",
    "\n",
    "# Download each file\n",
    "# Base URL for file downloads\n",
    "\n",
    "print(f\"üì• Downloading {mosaiks_data_url}...\")\n",
    "download_file(mosaiks_data_url, zip_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# unzip all downloaded files\n",
    "for zip_file in glob.glob(os.path.join(mosaiks_data_path, \"*.zip\")):\n",
    "    # Open the zip file\n",
    "    with zipfile.ZipFile(zip_file, \"r\") as z:\n",
    "        print(f\"Extracting {zip_file} to {mosaiks_data_path}...\")\n",
    "        # Extract all the contents into the extraction directory\n",
    "        z.extractall(mosaiks_data_path)\n",
    "print('done unzipping!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Wealth Data Labels to Merge with MOSAIKS Embeddings\n",
    "Now we need to match up our locations of our wealth data with the nearest MOSAIKS locations on the global grid of embeddings. We'll do that with some decimal rounding tricks. Some labels will share the same nearest neighbor, so we'll just pick the first one that has a non-null label for `asset_index`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load labels dataset\n",
    "print(\"Reading in data labels...\")\n",
    "df = pd.read_csv(labels_filename)\n",
    "\n",
    "\n",
    "# drop rows that have a NaN for asset_index\n",
    "print(\"Dropping any rows that don't have a value for asset index...\")\n",
    "before_len = len(df)\n",
    "df = df.dropna(subset=[\"asset_index\"])\n",
    "print(f\"Started with {before_len} rows, now have {len(df)} rows\")\n",
    "\n",
    "# Assign coordinates to nearest tile centroid\n",
    "print(\"Finding nearest MOSAIKS embedding locations...\")\n",
    "# for 1 degree\n",
    "if decimal_place == 0:\n",
    "    df[\"lon2\"] = round(round(df[\"lon\"] + 0.5, 0) - 0.5, 1)\n",
    "    df[\"lat2\"] = round(round(df[\"lat\"] + 0.5, 0) - 0.5, 1)\n",
    "# for 1/4 degree\n",
    "if decimal_place == 0.25:\n",
    "    df[\"lon2\"] = round((round(df[\"lon\"]*4 + 0.5, 0) - 0.5)/4, 3)\n",
    "    df[\"lat2\"] = round((round(df[\"lat\"]*4 + 0.5, 0) - 0.5)/4, 3)\n",
    "\n",
    "# keep just one row for every unique Lat, Lon combination\n",
    "print(\"Dropping labels that share the same embedding location...\")\n",
    "before_len = len(df)\n",
    "df = df.drop_duplicates(subset=[\"lat2\", \"lon2\"], keep=\"first\")\n",
    "print(f\"Started with {before_len} rows, now have {len(df)} rows\")\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "print(\"Converting to a geodataframe and exporting to csv...\")\n",
    "gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df[\"lon2\"], df[\"lat2\"]), crs=\"EPSG:4326\")\n",
    "gdf.to_csv(labels_centered_filename, index=False)\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize the data. It should appear in a grid pattern at 0.25 degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(12, 6))\n",
    "\n",
    "m = Basemap(projection='cyl', resolution='c', ax=ax)\n",
    "m.drawcoastlines()\n",
    "ax.scatter(df[\"lon2\"], df[\"lat2\"], c=df[\"asset_index\"], s=1)\n",
    "ax.set_title('Mean Asset Index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Embeddings with Labels\n",
    "Now that the data points have been moved to the centroids of the clusters, we can merge the embeddings with the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "mosaiks = pd.read_csv(mosaiks_embeddings_file_path)\n",
    "mosaiks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(labels_centered_filename)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# merge the two dataframes on the Lat, Lon columns from the labels and the lat, lon columns from the mosaiks\n",
    "mosaiks = mosaiks.rename(columns={\"lat\": \"lat2\", \"lon\": \"lon2\"})\n",
    "df_merged = df.merge(mosaiks, on=[\"lat2\", \"lon2\"], how=\"left\")\n",
    "\n",
    "# save this as a csv\n",
    "df_merged.to_csv(labels_merged_filename, index=False)\n",
    "\n",
    "# inspect the data\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep Data for Predicting Wealth Using Regression\n",
    "1. Drop any rows with missing label values\n",
    "2. Split into test / validation / train datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# read in the data\n",
    "df = pd.read_csv(labels_merged_filename)\n",
    "\n",
    "# keep just the columns we need\n",
    "df = df[[col for col in df.columns if col.startswith(\"X_\")] + [\"asset_index\"]]\n",
    "# check for NaNs\n",
    "print(\"Number of NaNs in df: \", df.isna().sum())\n",
    "# drop rows with NaNs\n",
    "df = df.dropna()\n",
    "# check the shape again\n",
    "print(\"Shape of df: \", df.shape)\n",
    "\n",
    "# split the data into train and test sets\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "# split the test set into val and test sets\n",
    "df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=42)\n",
    "# check the shapes of the data\n",
    "print(\"Shape of df_train: \", df_train.shape)\n",
    "print(\"Shape of df_val: \", df_val.shape)\n",
    "print(\"Shape of df_test: \", df_test.shape)\n",
    "\n",
    "\n",
    "# separate the data into X and y\n",
    "y_train = df_train[\"asset_index\"]\n",
    "y_val = df_val[\"asset_index\"]\n",
    "y_test = df_test[\"asset_index\"]\n",
    "\n",
    "# x is all the data in columns with names formatted like \"X_1\"\n",
    "X_train = df_train[[col for col in df.columns if col.startswith(\"X_\")]]\n",
    "X_val = df_val[[col for col in df.columns if col.startswith(\"X_\")]]\n",
    "X_test = df_test[[col for col in df.columns if col.startswith(\"X_\")]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Linear Classifier using Ridge Regression\n",
    "Now that we have our embeddings and our labels, we can run a linear classifier on top of the embeddings to see how well each model does at predicting poverty and wealth. We will use Ridge regression to do this, since it is a linear model that can handle high dimensional data well. It uses L2 regularization to prevent overfitting, penalizing large weights. You can learn more about it in [this Medium post](https://medium.com/@msoczi/ridge-regression-step-by-step-introduction-with-example-0d22dddb7d54) or in this tutorial from [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2016/01/ridge-lasso-regression-python-complete-tutorial/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define alpha values to loop over\n",
    "alpha_values = [10**i for i in range(-5, 4)]  # Corrected alpha values from 0.001 to 1000\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, alpha in enumerate(alpha_values):\n",
    "    ridge_model = Ridge(alpha=alpha)\n",
    "    ridge_model.fit(X_train, y_train)\n",
    "    y_pred = ridge_model.predict(X_val)\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "    \n",
    "    # Create scatter plot using Seaborn\n",
    "    sns.scatterplot(x=y_val, y=y_pred, ax=axes[i], s=10)\n",
    "    axes[i].plot([-4, 4], [-4, 4], \"r--\")  # 45-degree line\n",
    "    \n",
    "    # Fit a regression line with confidence interval\n",
    "    sns.regplot(x=y_val, y=y_pred, ax=axes[i], scatter=False, ci=95, line_kws={\"color\": \"blue\"})\n",
    "    \n",
    "    axes[i].set_title(f\"Validation Performance, Alpha={alpha}, R¬≤={r2:.2f}\")\n",
    "    axes[i].set_xlabel(\"Asset Index (True)\")\n",
    "    axes[i].set_ylabel(\"Asset Index (Predicted)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the top performing value for alpha on the validation dataset is 0.01. For a final test of the model, let's run this regression model trained on the test data, tuned on the validation, and see how well it performs on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Select the best alpha value from the validation data\n",
    "alpha = 0.01\n",
    "\n",
    "# Train Ridge regression model on training data\n",
    "ridge_model = Ridge(alpha=alpha)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "coefficients = ridge_model.coef_\n",
    "\n",
    "# Use coefficients from training for inference on test data\n",
    "y_pred = X_test @ coefficients + ridge_model.intercept_\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Create scatter plot using Seaborn\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(x=y_test, y=y_pred, s=10)\n",
    "plt.plot([-4, 4], [-4, 4], \"r--\")  # 45-degree line\n",
    "\n",
    "# Fit a regression line with confidence interval\n",
    "sns.regplot(x=y_test, y=y_pred, scatter=False, ci=95, line_kws={\"color\": \"blue\"})\n",
    "\n",
    "plt.title(f\"Test Data Peformance, Alpha={alpha}, R¬≤={r2:.2f}\")\n",
    "plt.xlabel(\"Asset Index (True)\")\n",
    "plt.ylabel(\"Asset Index (Predicted)\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Wealth Prediction\n",
    "Now that we have a model that can predict wealth, let's use it to predict wealth across the entire globe. We will use the MOSAIKS API to get embeddings for the entire world, and then use our model to predict wealth for each location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Retrain Ridge regression model on training data\n",
    "print(\"Retraining ridge model on training data...\")\n",
    "alpha = 0.01\n",
    "ridge_model = Ridge(alpha=alpha)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "coefficients = ridge_model.coef_\n",
    "\n",
    "print(\"reading in the global dataset...\")\n",
    "# read in the global dataset of mosaiks embeddings\n",
    "mosaiks = pd.read_csv(mosaiks_embeddings_file_path)\n",
    "print(\"read in dataset\")\n",
    "\n",
    "lat_global = mosaiks[\"lat\"]\n",
    "lon_global = mosaiks[\"lon\"]\n",
    "# pull out the embeddings from the dataframe, columns that begin with \"X_\"\n",
    "X_global = mosaiks.drop(columns=[\"lat\", \"lon\", \"continent\"])\n",
    "print(\"extracted just embeddings\")\n",
    "# Select the best alpha value from the validation data\n",
    "alpha = 0.01\n",
    "# Train Ridge regression model on training data\n",
    "coefficients = ridge_model.coef_\n",
    "print(\"fitted ridge model to training data\")\n",
    "\n",
    "# Use coefficients from training for inference on test data\n",
    "print(\"running inference on the global data...\")\n",
    "y_pred_global = X_global @ coefficients + ridge_model.intercept_\n",
    "print(\"inference complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the global predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Merge predictions with lat/lon, name prediction column \"pred\"\n",
    "df_global = pd.concat([lat_global, lon_global, pd.Series(y_pred_global)], axis=1)\n",
    "df_global.columns = [\"lat\", \"lon\", \"pred\"]\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "gdf_global = gpd.GeoDataFrame(df_global, geometry=gpd.points_from_xy(df_global[\"lon\"], df_global[\"lat\"]), crs=\"EPSG:4326\")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(12, 6))\n",
    "\n",
    "m = Basemap(projection='cyl', resolution='c', ax=ax)\n",
    "m.drawcoastlines()\n",
    "\n",
    "sc = ax.scatter(gdf_global[\"lon\"], gdf_global[\"lat\"], c=gdf_global[\"pred\"], s=2, cmap='viridis')\n",
    "cb = fig.colorbar(sc, ax=ax, orientation='vertical', label='Predicted Mean Asset Index')\n",
    "\n",
    "ax.set_title('Mean Asset Index Global Prediction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "1. Answer the following questions based on the regression figures above:\n",
    "    - What's going on with the small alpha values? Why are most data points squished up towards the top? Why is there such a strong outlier?\n",
    "    - What do you think is going on with the large alpha values? Why does the R-squared value decrease again? \n",
    "    - The red line shows a 45-degree line of a theoretical perfect fit. The blue line shows an approximation of the actual fit. Why is the blue line consistently flatter than the red line? What does this mean for our model? How could we improve that?\n",
    "\n",
    "2. What do you make of our global inference map? Does it match up with your intuition of wealthy and poor areas of the world? Do you think this is a valid approach for making a global inference? What pitfalls might this approach be prone to?\n",
    "\n",
    "3. Pick another variable from the SustainBench poverty dataset and repeat the analysis above. Do you see an even stronger correlation with MOSAIKS embeddings? A weaker one? Why do you think that is?\n",
    "\n",
    "\n",
    "## Bonus Assignment 1: Redo Analysis Higher Resolution Embeddings\n",
    "Instead of using the low-resolution embeddings aggregated to every 0.25 degrees, we have the original embeddings at every 0.01 degrees for each of the points in the SustainBench dataset in Kaggle [here](https://www.kaggle.com/datasets/isaiahlg/mosaiks-embeddings-for-sustainbench-wealth-mapping/). Thanks to [Luke Sherman](https://www.globalpolicy.science/luke-sherman) from Stanford for pulling them for us even though the MOSAIKS API was down. How does the spatial resolution of the DHS clusters and the dense MOSAIKS embeddings compare? Repeat the analysis using the dense embeddings. How does the accuracy compare with the coarse resolution embeddings?\n",
    "\n",
    "## Bonus Assignment 2: Geospatial Errors\n",
    "Associate the errors in the model above with the original geospatial datapoints in the SustainBench dataset and plot them on a map. Do you see any patterns? Are there any areas of the map that perform worse than others? If so, what do you think you could do to reduce the geographic bias of the model?\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6635435,
     "sourceId": 10707414,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30887,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "tg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
