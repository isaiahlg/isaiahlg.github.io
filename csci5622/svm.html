<!DOCTYPE html>
<html>

<head>
    <title>Isaiah LG - ML/SVMs</title>
    <link rel="stylesheet" href="/style.css">
    <link rel="icon" href="/assets/baobab.png">
    <script src="https://kit.fontawesome.com/f4ea09cda5.js" crossorigin="anonymous"></script>
    <!-- For converting markdown to html -->
    <script type="module" src="https://md-block.verou.me/md-block.js"></script>
    <!-- for rendering equations from TeX to HTML -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
    <button class="back-button" onclick="window.location.href='/csci5622/home.html';"><i class="fa-solid fa-arrow-left"></i></button>
    <div class="content">
        <div class="textbox">
            <!-- Assignment
            (a) Overview: Here, describe SVMs, why they are linear separators, how the kernel works, why the dot product is so critical to the use of the kernel, and what the polynomial and rbf kennel function look like. Also show an example of taking a 2D points and a polynomial kernel with r = 1 and d = 2 and "casting" that point into the proper number of dimensions.  Have at least two images that assist in your overview of SVMs. 

            (b) Data Prep. All models and methods require specific data formats. Supervised modeling requires first that you have labeled data. Next, it requires that you split your data into a Training Set (to train/build) the model, and a Testing Set to test the accuracy of your model. ONLY labeled data can be used for supervised methods. Explain this and show an image of the sample of data you plan to use. LINK to the sample of data as well. Also include information and a small image of the Training Set and Testing set and explain how you created them and why they are (and must be) disjoint. SVMs can only work on labeled numeric data. Be sure to include this fact and explain why. 

            (c) Code. Use Python. Create code that performs SVM modeling (classification) on your dataset. LINK to the code. 

            (d) Results. Discuss, illustrate, describe, and visualize the results. Include the confusion matrix and the accuracy.  You must use at least 3 different kernels. Try out different costs with each of the three kernels. Include, for each kernel (assuming an appropriate cost) a confusion matrix. Create at least one visualization. Compare the kernels. Which was best?

            (e) Conclusions. What did you learn (and/or what can you predict here) that pertains to your topic? -->
            <md-block>
            # Support Vector Machines (SVMs)
            
            ## Overview
            Support Vector Machines (SVMs) are a type of machine learning algorithm that are commonly used for classification tasks. At their core, SVMs are linear separators, which means they use a linear boundary to separate two classes of data in a feature space. To work, this requires that the data be linearly separable. To create that separator, SVMs find a multi-dimensional hyperplane to separate one group of data from the rest. That hyperplane is the support vector machine.  The "support vectors" are the data points that are closest to this separator. They help define the boundary more than the rest of data points.  
            
            <figure>
                <img class="figure" src="/csci5622/figures/svm/margin.png">
                <figcaption>A diagram illustrating how the SVM algorithm finds a hyperplane (here just a line) that separates the two categories based on their attributes X1 and X2. All values on one side belong to one category, and all values on the other side do not. To make this separator as effective as possible at classification, we want to maximize the size of the margin. (Image from Analytics Vidhya, 2021)</figcaption>
            </figure>

            However, in practice, the data may not be linearly separable, which is where the kernel comes in. The kernel is a function that takes in two data points and returns a similarity score. By using the kernel trick, SVMs can map the data into a higher-dimensional space where the data may become linearly separable. Take the example below. If we have 1 dimensional data, the only way to separate it is with a point. However, if that data is mirrored as we have below, this becomes impossible. To fix this here, we can add a polynomial kernel by squaring each data point, and defining the distance between two points as the difference of squares. We are effectively casting the points onto a 2D parabola, which allows us to then draw a line (a 1D hyperplane) separating the categories. 

            <figure>
                <img class="figure" src="/csci5622/figures/svm/kernel.png">
                <figcaption>A diagram illustrating how a kernel can make data linearly separable by effectively casting it into a higher dimension (Image by Kovenko, 2020)</figcaption>
            </figure>

            The dot product is critical to the use of the kernel because it measures the similarity between two data points in the higher-dimensional space. The kernel function can be thought of as a way of implicitly computing the dot product in this higher-dimensional space, without actually having to perform the expensive computations.

            There are several types of kernel functions that can be used with SVMs, but two common ones are the polynomial kernel and the radial basis function (RBF) kernel. The polynomial kernel maps the data into a higher-dimensional space using a polynomial function, while the RBF kernel maps the data into an infinite-dimensional space using a Gaussian function.
            
            </md-block>
            The polynomial kernel has the form \(K(x, y) = (\hat{x} • \hat{y} + r)^d\), where x and y are two data vectors, r is a constant, and d is the degree of the polynomial. The RBF kernel has the form \(K(x, y) = e^{-\gamma ||x - y||^2}\), where gamma is a hyperparameter that controls the shape of the kernel. This kernel is most useful for data that has radial symmetry. Let's write out an example of a polynomial kernel for 2-dimensional data where r = 1 and d = 2. Let's say we have two vectors \(\hat{x} = [x_1, x_2]\), and \(\hat{y} = [y_1, y_2]\).
            \[K(\hat{x},\hat{y}) = (\hat{x} • \hat{y} + r)^d \]
            \[K(\hat{x},\hat{y}) = (\hat{x} • \hat{y} + 1)^2 \]
            \[K(\hat{x},\hat{y}) = (x_1•y_1 + x_2•y_2 + 1)^2 \]
            \[K(\hat{x},\hat{y}) = x_1^2 y_1^2 + x_2^2 y_2^2 + 2 x_1 y_1 x_2 y_2 + x_1 y_1 + x_2 y_2 + 1 \]

            This result is the same as taking the dot product of two higher dimensional vectors that can be expressed in terms of the original vector components: 
            \[\hat{x'} = [x_1^2, x_2^2, \sqrt{2} x_1 x_2, x_1, x_2, 1]\]
            \[\hat{y'} = [y_1^2, y_2^2, \sqrt{2} y_1 y_2, y_1, y_2, 1]\]

            Because this expression can be expressed as a dot product, it can be substited into the dual form of the optimization function. Techniques like this can make data linearly separable without actually having to cast the data into higher dimensions. The kernel allows us to find the distance between two points in these higher dimensions without actually casting them, dramatically improving performance of SVMs.

            Overall, SVMs are a powerful tool for classification tasks, and the kernel trick allows them to handle complex, non-linearly separable data.
            <md-block>

            ## Data Prep
            Preparing the data for SVMs is relatively straightforward. Since SVMs are a supervised learning method, one must begin with labeled data. The other key consideration is that SVMs can only be performed on numerical data, since each data point must exist within n-dimensional space to be separable. For this analysis, the variable we will use as the label is again, the wealth index quantile (hv270). We will try to determine with economic class a given household belongs to based on their responses to a set of numerical variables.

            First, the dataset is cut down from over 3000 to just 50ish economic and demographic variables of interest to make the dataset easier to handle. Next, only the variables that can be converted to numerical responses are included. These variables were selected from the decision tree analysis in order of variable importance. Some basic demographic variables were also included since they were easily converted to numeric values. The final list of variables used is:
            - hv243a (Mobile telephone)
            - hv244 (Ag. Landowner)
            - hv209 (Refridgerator)
            - hv208 (Television)
            - hv243b (Watch)
            - hv206 (Electricity)
            - hv207 (Radio)
            - hv010 (Number of eligible women in household)
            - hv011 (Number of eligible men in household)
            - hv012 (Number of de jure members)
            - hv014 (Number of children 5 and under (de jure))
            - hv216 (Number of rooms used for sleeping)
            
            Below is a screenshot of the data after it has been filtered down to just the columns of interest, but before any of the values have been changed. The data can be found on GitHub [here](https://github.com/isaiahlg/csci5622mod4/blob/main/exports/sl19svm.csv).
            <figure><img class="figure" src="/csci5622/figures/svm/datapre.png"></figure>

            Next response values are then converted from categorical to numerical values, such as "no" to 0 and "yes" to 1. This allows us to be able to change the datatype of of each of the columns to numeric data. Next, the data are normalized with a standard deviation of 1 and a mean of 0 to make it easier for the support vector machine algorithm to fit the data. Below is a screenshot of the data after it has been scaled. A copy of this dataframe can be found on GitHub [here](https://github.com/isaiahlg/csci5622mod4/blob/main/exports/sl19scaled.csv).
            <figure><img class="figure" src="/csci5622/figures/svm/datascaled.png"></figure>

            To better see the way in which the data are separable based on the variables used in this analysis, we can run Principle Component Analysis for 2 components and then plot the 2D results. We can see a good amount of clustering, but there is still a good deal of overlap. This foreshadows some challenges that the SVMs might have in creating separability in the data.
            
            <figure>
                <img class="figure" src="/csci5622/figures/svm/clusters.png"> 
                <figcaption>A scatterplot of the two principle components of the data vectors, pc1 and pc2. The wealth index of the household is indicated with color. There is a decent amount of clustering, though there remains a good deal of overlap between categories.</figcaption>
            </figure>
            
            Now that we have scaled data, we then separate it into training and testing data so that we can evaluate the performance of each of our SVMs. In this case, an 80/20 split is used for training and testing respectively. Labels are also removed and stored separately from the testing data and training data to allow for the training and evaluation of each.  Below is a screenshot of the training (left) and testing (right) data. Note that the row indexes are different and do not have overlap to ensure we are testing with data the model has never seen before.
            <figure>
                <img class="figure2" src="/csci5622/figures/svm/datatrain.png">
                <img class="figure2" src="/csci5622/figures/svm/datatest.png">
            </figure>

            Now that the data is ready for SVMs, we fit an SVM to the training data and then evaluate it on the test data a total of 12 times: 3 different kernels with 4 difference cost values for each kernel. The kernels used are linear, radial basis function (RBF), and polynomial. The 4 different cost values used are 1, 10, 100, and 1000. The cost value determines how much an SVM should be allowed to "bend" with the data. Extra bending may lead to the correct classification of more points in the training dataset, but at the cost of a narrower margin. This can lead to reduced performance on the testing dataset. It it analogous to the overfitting of data in other machine learning models such as regression. Running each of these iterations allows us to compare various models to see what is best for our dataset. 

            ## Code
            Find all of the code use to clean the data and run the SVMs in Python on GitHub [here](https://github.com/isaiahlg/csci5622mod4/blob/main/svm.py)
            <figure><img class="figure" src="/csci5622/figures/svm/code.png"></figure>

            ## Results

            Below, you can find the results for each of the 12 runs from 4 different cost values on 3 different kernels.
            
            ### Linear Kernel
            The linear kernel performed quite well overall, correctly classifying about 55.3% of the data. It performed ever so slightly better with C=1000 at 55.4%, but this difference is so small, we cannot conclude that it ran better. It should be noted that the SVM fitting function threw a warning on the default number of maximum iterations of 1000 that "the model had not yet converged". Given this, the number of maximum iterations was increased to 100,000. This allowed the models for C=1 and C=10 to converge, but the warning persisted for C=100 and C=1000. Given the performance cost of increasing both the cost and the number of maximum iterations, this was the highest values used. As is, it still took the model over 10 minutes to fit the data with a linear kernel for C=1000 and 100,000 maximum iterations.
            <figure>
                <img class="figure2" src="/csci5622/figures/svm/linear1.png">
                <img class="figure2" src="/csci5622/figures/svm/linear10.png">
            </figure>
            <figure>
                <img class="figure2" src="/csci5622/figures/svm/linear100.png">
                <img class="figure2" src="/csci5622/figures/svm/linear1000.png">
            </figure>
            
            ### Radial Basis Function (RBF) Kernel
            For the radial basis function kernel, we actually see a small but consistent jump in performance, maxing out with C=10 at 56.3% This one actually dips down in accuracy significantly as the cost parameter is increased to 1000. 
            <figure>
                <img class="figure2" src="/csci5622/figures/svm/rbf1.png">
                <img class="figure2" src="/csci5622/figures/svm/rbf10.png">
            </figure>
            <figure>
                <img class="figure2" src="/csci5622/figures/svm/rbf100.png">
                <img class="figure2" src="/csci5622/figures/svm/rbf1000.png">
            </figure>

            ### Polynomial Kernel (3rd Degree)
            Here, we see the best overall performance of any of the three kernels. It peaks at 56.6% accuracy with C=10, and then decreases down to 56.1% and 55.4% with C=100 and 1000 respectively. This is a very similar relationship between accuracy and cost that was seen with the RBF kernel. 
            <figure>
                <img class="figure2" src="/csci5622/figures/svm/poly1.png">
                <img class="figure2" src="/csci5622/figures/svm/poly10.png">
            </figure>
            <figure>
                <img class="figure2" src="/csci5622/figures/svm/poly100.png">
                <img class="figure2" src="/csci5622/figures/svm/poly1000.png">
            </figure>

            ## Conclusions
            The key takeaway here is that the top performing SVM model is the one was that used the polynomial kernel with the 3rd degree and C=10. This is likely because there was a high degree over overlap in the data that the polynomial kernel was able to create some extra separation in by adding additional dimensionality. We can plot the results of all of the runs on a single 3 x 4 heatmap.

            <figure><img class="figure" src="/csci5622/figures/svm/accuracies.png"></figure>

            One thing that isn't depicted here is that all of this analysis was actually performed as well with just 5 of the variables listed above, the ones relating to demographics. However, the prediction accuracy of the SVMs was between 20-30%. This is pretty abysmal performance given that there are only 5 classes, and the chance of randomly guessing one correctly is 20%. Included below is one of the confusion matrices from this analysis for reference.
            <figure>
                <img class="figure2" src="/csci5622/figures/svm/linearc1.png">
                <img class="figure2" src="/csci5622/figures/svm/linearc10.png">
            </figure>
            <figure>
                <img class="figure2" src="/csci5622/figures/svm/linearc100.png">
                <img class="figure2" src="/csci5622/figures/svm/linearc1000.png">
            </figure>
            These commically bad classification models give a similar insight as that had during clustering --- demographic variables along do not make for very strong predictors of economic wealth. The accuracy of these models jumped way up into the 50s when high-importance variables from the decision tree analysis were added in here. This highlights a key limitation of support vector machines -- the fact that they do not perform variable selection and that they cannot handle categorical data. For a dataset like this one that has many different categorical variables, one is left with either one-hot encoding all of these (unwieldy), or simply ommitting them. It appears that the correct selection of variables has a much higher impact on the performance of the model than the selection of a kernel or the right hyperparameters.
           
            ## References
            Saini, Anshul. 2021. "Support Vector Machine(SVM): A Complete guide for beginners." Analytics Vidhya.
                https://www.analyticsvidhya.com/blog/2021/10/support-vector-machinessvm-a-complete-guide-for-beginners/
            
            Kovenko, Volodymyr. 2020. "SVM (Support Vector Machines)". DS/ML Course.
                https://machine-learning-and-data-science-with-python.readthedocs.io/en/latest/assignment4_sup_ml.html
            </md-block>
        </div>
    </div>
</body>
</html>