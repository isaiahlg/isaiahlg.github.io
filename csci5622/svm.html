<!DOCTYPE html>
<html>

<head>
    <title>Isaiah LG - ML/SVMs</title>
    <link rel="stylesheet" href="/style.css">
    <script src="https://kit.fontawesome.com/f4ea09cda5.js" crossorigin="anonymous"></script>
    <!-- For converting markdown to html -->
    <script type="module" src="https://md-block.verou.me/md-block.js"></script>
    <!-- for rendering equations from TeX to HTML -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
    <button class="back-button" onclick="window.location.href='/csci5622/home.html';"><i class="fa-solid fa-arrow-left"></i></button>
    <div class="content">
        <div class="textbox">
            <!-- Assignment
            (a) Overview: Here, describe SVMs, why they are linear separators, how the kernel works, why the dot product is so critical to the use of the kernel, and what the polynomial and rbf kennel function look like. Also show an example of taking a 2D points and a polynomial kernel with r = 1 and d = 2 and "casting" that point into the proper number of dimensions.  Have at least two images that assist in your overview of SVMs. 

            (b) Data Prep. All models and methods require specific data formats. Supervised modeling requires first that you have labeled data. Next, it requires that you split your data into a Training Set (to train/build) the model, and a Testing Set to test the accuracy of your model. ONLY labeled data can be used for supervised methods. Explain this and show an image of the sample of data you plan to use. LINK to the sample of data as well. Also include information and a small image of the Training Set and Testing set and explain how you created them and why they are (and must be) disjoint. SVMs can only work on labeled numeric data. Be sure to include this fact and explain why. 

            (c) Code. Use Python. Create code that performs SVM modeling (classification) on your dataset. LINK to the code. 

            (d) Results. Discuss, illustrate, describe, and visualize the results. Include the confusion matrix and the accuracy.  You must use at least 3 different kernels. Try out different costs with each of the three kernels. Include, for each kernel (assuming an appropriate cost) a confusion matrix. Create at least one visualization. Compare the kernels. Which was best?

            (e) Conclusions. What did you learn (and/or what can you predict here) that pertains to your topic? -->
            <md-block>
            # Support Vector Machines (SVMs)
            
            ## Overview
            Support Vector Machines (SVMs) are a type of machine learning algorithm that are commonly used for classification tasks. At their core, SVMs are linear separators, which means they use a linear boundary to separate two classes of data in a feature space. To work, this requires that the data be linearly separable. To create that separator, SVMs find a multi-dimensional hyperplane to separate one group of data from the rest. That hyperplane is the support vector machine.  The "support vectors" are the data points that are closest to this separator. They help define the boundary more than the rest of data points.  
            
            <figure>
                <img class="figure" src="/csci5622/figures/svm/margin.png">
                <figcaption>A diagram illustrating how the SVM algorithm finds a hyperplane (here just a line) that separates the two categories based on their attributes X1 and X2. All values on one side belong to one category, and all values on the other side do not. To make this separator as effective as possible at classification, we want to maximize the size of the margin. (Image from Analytics Vidhya, 2021)</figcaption>
            </figure>

            However, in practice, the data may not be linearly separable, which is where the kernel comes in. The kernel is a function that takes in two data points and returns a similarity score. By using the kernel trick, SVMs can map the data into a higher-dimensional space where the data may become linearly separable. Take the example below. If we have 1 dimensional data, the only way to separate it is with a point. However, if that data is mirrored as we have below, this becomes impossible. To fix this here, we can add a polynomial kernel by squaring each data point, and defining the distance between two points as the difference of squares. We are effectively casting the points onto a 2D parabola, which allows us to then draw a line (a 1D hyperplane) separating the categories. 

            <figure>
                <img class="figure" src="/csci5622/figures/svm/kernel.png">
                <figcaption>A diagram illustrating how a kernel can make data linearly separable by effectively casting it into a higher dimension (Image by Kovenko, 2020)</figcaption>
            </figure>

            The dot product is critical to the use of the kernel because it measures the similarity between two data points in the higher-dimensional space. The kernel function can be thought of as a way of implicitly computing the dot product in this higher-dimensional space, without actually having to perform the expensive computations.

            There are several types of kernel functions that can be used with SVMs, but two common ones are the polynomial kernel and the radial basis function (RBF) kernel. The polynomial kernel maps the data into a higher-dimensional space using a polynomial function, while the RBF kernel maps the data into an infinite-dimensional space using a Gaussian function.
            
            </md-block>
            The polynomial kernel has the form \(K(x, y) = (\hat{x} • \hat{y} + r)^d\), where x and y are two data vectors, r is a constant, and d is the degree of the polynomial. The RBF kernel has the form \(K(x, y) = e^{-\gamma ||x - y||^2}\), where gamma is a hyperparameter that controls the shape of the kernel. This kernel is most useful for data that has radial symmetry. Let's write out an example of a polynomial kernel for 2-dimensional data where r = 1 and d = 2. Let's say we have two vectors \(\hat{x} = [x_1, x_2]\), and \(\hat{y} = [y_1, y_2]\).
            \[K(\hat{x},\hat{y}) = (\hat{x} • \hat{y} + r)^d \]
            \[K(\hat{x},\hat{y}) = (\hat{x} • \hat{y} + 1)^2 \]
            \[K(\hat{x},\hat{y}) = (x_1•y_1 + x_2•y_2 + 1)^2 \]
            \[K(\hat{x},\hat{y}) = x_1^2 y_1^2 + x_2^2 y_2^2 + 2 x_1 y_1 x_2 y_2 + x_1 y_1 + x_2 y_2 + 1 \]

            This result is the same as taking the dot product of two higher dimensional vectors that can be expressed in terms of the original vector components: 
            \[\hat{x'} = [x_1^2, x_2^2, \sqrt{2} x_1 x_2, x_1, x_2, 1]\]
            \[\hat{y'} = [y_1^2, y_2^2, \sqrt{2} y_1 y_2, y_1, y_2, 1]\]

            Because this expression can be expressed as a dot product, it can be substited into the dual form of the optimization function. Techniques like this can make data linearly separable without actually having to cast the data into higher dimensions. The kernel allows us to find the distance between two points in these higher dimensions without actually casting them, dramatically improving performance of SVMs.

            Overall, SVMs are a powerful tool for classification tasks, and the kernel trick allows them to handle complex, non-linearly separable data.
            <md-block>

            ## Data Prep
            Preparing the data for SVMs is relatively straightforward. The key driver is that SVMs can only be performed on numerical data, since each data point must be able to be plotted into n-dimensional space. For this analysis, the variable we will categorize by is again, the wealth index quantile (hv270). We will try to determine with economic class a given household belongs to based on their responses to a set of numerical variables.

            First, the dataset is cut down from over 3000 to just 50ish economic and demographic variables of interest to make the dataset easier to handle. Response values are then converted from categorical to numerical values, such as "no" to 0 and "yes" to 1. Next, only the variables that can be converted to numerical responses are included. These variables were selected from the decision tree analysis in order of variable importance. Some basic demographic variables were also included since they were easily converted to numeric values. The final list of variables used is:
            - hv243a (Mobile telephone)
            - hv244 (Ag. Landowner)
            - hv209 (Refridgerator)
            - hv208 (Television)
            - hv243b (Watch)
            - hv206 (Electricity)
            - hv207 (Radio)
            - hv010 (Number of eligible women in household)
            - hv011 (Number of eligible men in household)
            - hv012 (Number of de jure members)
            - hv014 (Number of children 5 and under (de jure))
            - hv216 (Number of rooms used for sleeping)

            Finally, the data are normalized with a standard deviation of 1 and a mean of 0 to make it easier for the support vector machine algorithm to fit the data. To better see the way in which the data cluster based on the variables used in this analysis, we can run Principle Component Analysis for 2 components and then plot the 2D results. We can a good amount of clustering, but still a good deal of overlap. This is likely indicative of how our SVM will be able to perform.
            
            <figure>
                <img class="figure" src="/csci5622/figures/svm/clusters.png"> 
                <figcaption>A scatterplot of the two principle components of the data vectors, pc1 and pc2. The wealth index of the household is indicated with color. There is a decent amount of clustering, though there remains a good deal of overlap between categories.</figcaption>
            </figure>
                
            Now that we have scaled data, we then separate it into training and testing data so that we can evaluate the performance of each of our SVMs. In this case, an 80/20 split is used for training and testing respectively. Labels are also removed from the testing data and training data to allow for the training and evaluation of each.
            
            Now that the data is ready for SVMs, we fit an SVM to the training data and then evaluate it on the test data a total of 12 times: 3 different kernels with 4 difference cost values for each kernel. The kernels used are linear, radial basis function (RBF), and polynomial. The 4 different cost values used are 1, 10, 100, and 1000. The cost value determines how much an SVM should be allowed to "bend" with the data. Extra bending may lead to the correct classification of more points in the training dataset, but at the cost of a narrower margin. This can lead to reduced performance on the testing dataset. It it analogous to the overfitting of data in other machine learning models such as regression. 

            ## Code
            Find all of the code use to clean the data and run the SVMs in Python on GitHub [here](https://github.com/isaiahlg/csci5622mod4/blob/main/svm.py)
            <figure><img class="figure" src="/csci5622/figures/svm/code.png"></figure>

            ## Results

            Below, you can find the results for each of the 12 runs from 4 different cost values on 3 different kernels.
            
            ### Linear Kernel
            <figure>
                <img class="figure2" src="/csci5622/figures/svm/linear1.png">
                <img class="figure2" src="/csci5622/figures/svm/linear10.png">
            </figure>
            <figure>
                <img class="figure2" src="/csci5622/figures/svm/linear100.png">
                <img class="figure2" src="/csci5622/figures/svm/linear1000.png">
            </figure>
            
            ### Radial Basis Function (RBF) Kernel
            <figure>
                <img class="figure2" src="/csci5622/figures/svm/rbf1.png">
                <img class="figure2" src="/csci5622/figures/svm/rbf10.png">
            </figure>
            <figure>
                <img class="figure2" src="/csci5622/figures/svm/rbf100.png">
                <img class="figure2" src="/csci5622/figures/svm/rbf1000.png">
            </figure>

            ### Polynomial Kernel (3rd Degree)
            <figure>
                <img class="figure2" src="/csci5622/figures/svm/poly1.png">
                <img class="figure2" src="/csci5622/figures/svm/poly10.png">
            </figure>
            <figure>
                <img class="figure2" src="/csci5622/figures/svm/poly100.png">
                <img class="figure2" src="/csci5622/figures/svm/poly1000.png">
            </figure>

           
            ## References
            Saini, Anshul. 2021. "Support Vector Machine(SVM): A Complete guide for beginners." Analytics Vidhya.
                https://www.analyticsvidhya.com/blog/2021/10/support-vector-machinessvm-a-complete-guide-for-beginners/
            
            Kovenko, Volodymyr. 2020. "SVM (Support Vector Machines)". DS/ML Course.
                https://machine-learning-and-data-science-with-python.readthedocs.io/en/latest/assignment4_sup_ml.html
            </md-block>
        </div>
    </div>
</body>
</html>