<!DOCTYPE html>
<html>

<head>
    <title>Isaiah LG - ML/ARM</title>
    <link rel="stylesheet" href="/style.css">
    <script src="https://kit.fontawesome.com/f4ea09cda5.js" crossorigin="anonymous"></script>
</head>

<body>
    <button class="back-button" onclick="history.back()"><i class="fa-solid fa-arrow-left"></i></button>
    <div class="content">
        <div class="textbox">
            <h1>Association Rule Mining</h1>
            <!-- 
                (a) Overview: Here, describe ARM, including measures like support, confidence, and lift. What are rules? 
                    What is the Apriori algorithm and how does it work?  Have two images. More is fine.
                (b) Data Prep. All models and methods require specific data formats. ARM requires ONLY unlabeled transaction data. 
                    Explain this and show an image of the sample of data you plan to use. LINK to the sample of data as well. 
                (c) Code. Use  R (only) to code ARM. LINK to the code.
                (d) Results. Discuss, illustrate, describe, and visualize the results. Include the top 15 rules for support, 
                    the top 15 for confidence, and the top 15 for lift. What thresholds did you use? Include at least 2 visualizations 
                    (networks) that show the associations you found. 
                (e) Conclusions. What did you learn that pertains to your topic? 
            -->
            <h3>Overview</h3>
            <p>
                Association Rule Mining (ARM) is a technique in machine learning that identifies relationships between items in large datasets. It involves finding patterns, or associations, between items that occur together frequently, and "mining" out rules that describe the data. ARM is perfect for transaction data where each transaction is unlabeled, just like a grocery store basket.
            </p>
            <figure>
                <img class="figure" src="/csci5622/figures/arm/basket.png">
                <figcaption>Rules are mined from transactional data to find items that appear frequently together.</figcaption>
            </figure>
            <p>
                A rule represents an if-then relationship between two sets of items with an antecedent on the left-hand side (the "if"), and the consequent on the right-hand side (the "then"). Each rule is characterized by a variety of metrics, the most common of which are support, confidence, and lift. These metrics are used to evaluate the strength and significance of the discovered  association rules. Support measures how often the sets of items appears in the dataset, while confidence measures the proportion of times that the consequent appears given the antecedent. The support will always be lower than the frequency of each item individually, and the confidence of a rule will always be greater than or equal to the rule. Lift measures the degree of correlation between the antecedent and consequent, accounting for the base frequency of both.

                The Apriori algorithm is a popular algorithm used in association rule mining. It works by generating a set of candidate itemsets and pruning them based on their support values. The algorithm starts with single items and incrementally builds larger itemsets  until no more frequent itemsets can be found. It prunes supersets by checking whether all subsets of a candidate itemset meet  the minimum support threshold. This reduces the search space and improves efficiency.
            </p>
            <figure>
                <img class="figure" src="/csci5622/figures/arm/apriori.png">
                <figcaption>The Apriori Algorithm makes rule mining much more effient as supersets of infrequent rules are pruned.</figcaption>
            </figure>
            <p>
                For this dataset, ARM is used to find associations between the various assets included in the DHS surveys, such as cell phones, bicycles, mosquito nets, and refridgerators.
            </p>

            <h3>Data Prep</h3>
            <p>
                As described above, Association Rule Mining requires data with unique items that are in transaction format. The first step towards this end for the DHS data is to filter for just columns of interest.  Of the 56 variables kept in the Data Cleaning phase, 21 are selected for association rule mining because they pertained to assets. They were:
                <ul>
                    <li>hv206: Has electricity</li>
                    <li>hv207: Has radio</li>
                    <li>hv208: Has television</li>
                    <li>hv209: Has refrigerator</li>
                    <li>hv210: Has bicycle</li>
                    <li>hv211: Has motorcycle/scooter</li>
                    <li>hv212: Has car/truck</li>
                    <li>hv221: Has telephone (land-line)</li>
                    <li>hv227: Has mosquito bed net for sleeping</li>
                    <li>hv243a: Has mobile telephone</li>
                    <li>hv243b: Has watch</li>
                    <li>hv243c: Has animal-drawn cart</li>
                    <li>hv243d: Has boat with a motor</li>
                    <li>hv243e: Has a computer</li>
                    <li>hv246a: Owns cattle</li>
                    <li>hv246b: Owns cows/ bulls</li>
                    <li>hv246c: Owns horses/ donkeys/ mules</li>
                    <li>hv246d: Owns goats</li>
                    <li>hv246e: Owns sheep</li>
                    <li>hv246f: Owns chickens/poultry</li>
                    <li>hv247: Has bank account</li>
                </ul> 
                The next step is to remove any rows that had NA, missing, or unknown values. In this dataset, that is any row with a value of 95 of higher. To do this, the dataframe is converted to integers and then a numeric filter is applied. Next, for consistency, all zero values are converted to NA, and all other non-zero values are converted to 1s. To continue, the integers are then converted to characters. The final transformation applied to the record data is  to substitute the 1s in each column for a word unique to each column that descibes the asset, such as  "electricity" or "goats". This left a neat dataframe pictured below:
            </p>
            <figure>
                <img class="figure" src="/csci5622/figures/arm/recorddata.png">
                <figcaption>The final state of the data while still in record format, ready to be changed into transaction data. This R data object is on Github <a href="https://github.com/isaiahlg/csci5622mod2/blob/main/proj/data/sl19words.rds">here</a>.
                </figcaption>
            </figure>
            <p>
                The next step is to convert the data into transaction data, which can fortunately completed in a single line in R. Beautiful. Below, find another snapshot of the data, this time in transaction format. In this format, it is essentially a list of assets that each house owns - a post-purchase market basket.
            </p>
            <figure>
                <img class="figure" src="/csci5622/figures/arm/txdata.png">
                <figcaption>The data is finally in transaction format and ready for rule mining. This R data object is on Github <a href="https://github.com/isaiahlg/csci5622mod2/blob/main/proj/data/tx.rds">here</a>.
                </figcaption>
            </figure>

            <h3>Code</h3>
            <p>
                All code for ARM is written in R, and can be found on Github <a href="https://github.com/isaiahlg/csci5622mod2/blob/main/proj/arm.Rmd">here</a>.
            </p>
            <!-- (d) Results. Discuss, illustrate, describe, and visualize the results. Include the top 15 rules for support, 
                    the top 15 for confidence, and the top 15 for lift. What thresholds did you use? Include at least 2 visualizations 
                    (networks) that show the associations you found. 
                (e) Conclusions. What did you learn that pertains to your topic?  -->
            <h3>Results</h3>
            <p>
                Before rules are even mined, one can plot the frequency of each of the items in the transaction data. In this case, this tell us what are the most commonly owned assets among the households surveyed. In the figure below, we can see that the most commonly owned assets by far are cell phones and bednets, followed by radio, chickens, and a watch. The next category might be considered luxuty items in Sierra Leone since fewer than 1 in 5 households have one: electricity, goats, television, and a bank account. Down close to 1 in 10 households, we have motorcycle, sheep, and fridge. Finally, the rest of the assets appear in fewer than 1 in 20 households: bicycle, computer, car, cows, and a boat. The last three assets have such low rates, they no longer seem like luxuries, but rather oddities: landline, animal cart, and horses. Really interesting stuff here.
            </p>
            <figure>
                <img class="figure" src="/csci5622/figures/arm/freq.png">
                <figcaption>The relative frequencies of assets among Sierra Leonean households.</figcaption>
            </figure>
            <p>
                Rule mining was a bit of an interative process in choosing the thresholds. The maximum value for support was just 0.5, and so the threshold was set relatively low at 0.15 to ensure rarer items with intersting associations could still be captured. Confidence values ranged quite high, so the threshold was set up to 0.5 to filter the total number of rules down close to 60--more than 45 but not much more. Finally, the rules have a minimum number of elements of two to avoid any null values on the left or right hand sides. Below are screenshots of our top rules sorted by their respective metrics:
            </p>
            <figure>
                <img class="figure" src="/csci5622/figures/arm/rulesSup1.png">
                <img class="figure" src="/csci5622/figures/arm/rulesSup2.png" style="margin-top: -5px;">
                <figcaption>The 15 rules with the highest support.</figcaption>
            </figure>
            <p>
                Unsurprisingly, the rule with the highest support is among bednets and cellphones, and vice version. The lift is just 1.02 though, meaning their appearance is relatively independent. The next highest support is between cell phones and radios, a logical pairing -- battery powered, connective electronics. A lift > 1 suggests they truly do pair together. Next we have chickens and bednets, which one can assume is an association found in rural areas. The only other item that we see emerge into the most common rules is watches. Not a lot of surpises nor insights here. Next, we can examine the rules with the highest confidence: 
            </p>
            <figure>
                <img class="figure" src="/csci5622/figures/arm/rulesConf1.png">
                <img class="figure" src="/csci5622/figures/arm/rulesConf2.png" style="margin-top: -5px;">
                <figcaption>The 15 rules with the highest confidence.</figcaption>
            </figure>
            <p>
                Here, we have some more telling rules! The highest confidence rule is that if a household has a television, there is a > 98% chance that they will have a cell phone. This is intuitive but the level of confidence is still impressive. Along a similar vein, the next high-confidence rule tell us that if a household has electricity, there's a > 97% chance they'll have a cell phone. Neither of these occur very often (about 1 in 6 households), but the confidence is so high. The next few rules are other combinations of assets that suggest a household will have a cell phone, such as a radio and a watch, or a bednet/watch combo. The top 12 confidence rules are all predicting the presence of cell phones. #13-15 are predicting the presence of bednets, given by radio/cell/chickens, as well as a watch. In short, these rules are telling us that cell phones and bednets not just widespread, but that they are much more prevalent among households with other, more rare assets. This suggests a sort of hierarchy among assets that could be useful for future analysis. Next, we can examine the rules with the highest lift: 
            </p>
            <figure>
                <img class="figure" src="/csci5622/figures/arm/rulesLift1.png">
                <img class="figure" src="/csci5622/figures/arm/rulesLift2.png" style="margin-top: -5px;">
                <figcaption>The 15 rules with the highest lift.</figcaption>
            </figure>
            <p>
                Finally, we can export the 10 most interesting rules based on lift into an interactive HTML-based visualization. The blue rectangles depict elements, connected by rules in red circles. The intensity of the color shows the lift of each rule. 
            </p>
            <figure>
                <a href="/csci5622/rulesVis.html">
                    <img class="figure" src="/csci5622/figures/arm/rulesVis.png">
                </a>
                <figcaption>An interactive visualization of the 10 rules with the highest lift. Click the image for the interactive version!</figcaption>
            </figure>
            
            
            <h3>Conclusions</h3>
            
            
            <h3>References</h3>
            <p class="reference">
                Data Camp. 2018. "Market Basket Analysis using R." https://www.datacamp.com/tutorial/market-basket-analysis-r
            </p>
            <p class="reference">
                Engati. 2021. "What is Apriori Algorithm?." https://www.engati.com/glossary/apriori-algorithm
            </p>
           
        </div>
    </div>

</body>

</html>