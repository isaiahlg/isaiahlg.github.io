<!DOCTYPE html>
<html>

<head>
    <title>Isaiah LG - ML/Clustering</title>
    <link rel="stylesheet" href="/style.css">
    <script src="https://kit.fontawesome.com/f4ea09cda5.js" crossorigin="anonymous"></script>
</head>

<body>
    <button class="back-button" onclick="window.location.href='/csci5622/home.html';"><i class="fa-solid fa-arrow-left"></i></button>
    <div class="content">
        <div class="textbox">            
            <h1>Clustering</h1>

            <h3>Overview</h3>
            Clustering is a set of unsupervised learning algorithms that serve to group like data together. In order to be able to group data by "similarity", all clustering methods must define a distance metric that gives the "distance" between any two points in the dataset, say A and B. The distance function can be any function that (a) is always >= 0, is 0 if A and B are the same point, and cannot be shortcutted through a third point C. That is, the distance D(A,B) must be >= D(A,C) + D(B,C). No wormholes! Common distance metrics include minkowski distances with p=1 (Manhattan Distance) and p=2 (Euclidean Distance), as well as consine similarity. One you have a distance metric, there are three main approaches to clustering: 
            <ol>
                <li><b>Partitional Clustering:</b> in this approach, "partitions" are created between groups that become clustered around centroids. Examples of this
                    are k-means, k-modes, and k-prototypes.
                </li>
                <li><b>Hierachical Clustering:</b> in this approach, elements are grouped together in a hierarchical tree, with similar elements belonging to the same
                branches. Algorithms can start from the top-down and divide (such as DIANA), or start from the bottom-up and agglomerate (such as AGNES).</li>
                <li><b>Density Clustering:</b> in this approach, elements are group with their closest neighbors iteratively. This way, clusters of points can be formed
                that span overlapping values of the various dimensions, but are shaped in a relatively continuous blob.</li>
            </ol>
            <figure>
                <img class="figure" src="/csci5622/figures/clustering/clustering.png">
                <figcaption>Examples of various types of clustering algorithms (Seif, 2018).</figcaption>
            </figure>
            In this project, clustering is used to determine how many meaningful groups there are among households in the survey based on the number of people that comprise the household, the number of rooms of the house, and a wealth index score. The outcome of this clustering will tell how many socio-demographicstrata there are within this dataset.
            <figure>
                <img class="figure" src="/csci5622/figures/clustering/slhome.jpg">
                <figcaption>A single-mother household in rural Sierra Leone (Wamala, 2022).</figcaption>
            </figure>
            
            <h3>Data Prep</h3>
            For this dataset, clustering is performed based on euclidean distance and based on cosine similarity. Both of these algorithms require continuous, numeric data for each of the dimensions, and so the data has to be cleaned. Of the 50 or so variables kept from the cleaning process, only 9 of them have clean numeric data. Of these variables, only 6 are determined to be of interest:
            <ol>
                <li>hv010: Number of eligible women in household</li>
                <li>hv011: Number of eligible men in household</li>
                <li>hv012: Number of de jure members</li>
                <li>hv014: Number of children 5 and under (de jure)</li>
                <li>hv216: Number of rooms used for sleeping</li>
                <li>hv270a: Wealth index for urban/rural</li>
            </ol>
            In this clustering analysis, demographics will be used to predict wealth index for urban rural, and so the final variable hv270a is removed as it is the "label". Below, find snapshots of the data post-processing.
            <figure>
                <img class="figure" src="/csci5622/figures/clustering/numdata.png">
                <figcaption>A snapshot of the dataset ready to be scaled for clustering. A copy of this numerical dataset can be found 
                    <a href="https://github.com/isaiahlg/csci5622mod2/blob/main/proj/data/sl19num.csv">here</a>.</figcaption>
            </figure>
            <figure>
                <img class="figure" src="/csci5622/figures/clustering/scaleddata.png">
                <figcaption>A snapshot of the dataset after normalization, ready for clustering. A copy of this scaled dataset can be found 
                    <a href="https://github.com/isaiahlg/csci5622mod2/blob/main/proj/data/sl19scaled.csv">here</a>.</figcaption>
            </figure>
            In order to see if there are any patterns in the clustering, a principal component analysis is run of the remaining 5 numerical variables down to 2 principal components. This allows for a 2D plot of the data. The label is added back into the PCA data frame to be graphed with color, with the results below:
            <figure>
                <img class="figure" src="/csci5622/figures/clustering/pcaclusters.png">
                <figcaption>A 2D plot of the principal components of the numerical data with the wealth index overlaid in color. Here one can see that 
                    the correlation between the demographic variables being used and wealth index is not very strong, since the colors are all a bit disperesed.
                </figcaption>
            </figure>
            <h3>Code</h3>
            Python is used to perform k-means clustering and principal component analysis with the code <a href="https://github.com/isaiahlg/csci5622mod2/blob/main/proj/kmeans.py">here</a>. R is used to perform hierarchical clustering with the code <a href="https://github.com/isaiahlg/csci5622mod2/blob/main/proj/hclust.Rmd">here</a>. 
            
            <h3>Results</h3>
            Clustering is run for k = [2,3,4,5]. Silhouette analysis is performed for each of these k-values, and the resulting charts are below:
            <figure>
                <img class="figure" src="/csci5622/figures/clustering/kmeans2.png">
                <figcaption>Clustering results for k=2, graphed along the two principal components. Unsurprisingly, the split is almost entirely along the first principal component.
                </figcaption>
            </figure>
            <figure>
                <img class="figure" src="/csci5622/figures/clustering/kmeans3.png">
                <figcaption>Clustering results for k=3, graphed along the two principal components. Here, the partitions are still primarily along the first principal component.
                </figcaption>
            </figure>
            <figure>
                <img class="figure" src="/csci5622/figures/clustering/kmeans4.png">
                <figcaption>Clustering results for k=4, graphed along the two principal components. Here, we see a fourth category emerge on top defined primarily by high values of the second principal component.
                </figcaption>
            </figure>
            <figure>
                <img class="figure" src="/csci5622/figures/clustering/kmeans5.png">
                <figcaption>Clustering results for k=5, graphed along the two principal components. Here, we seen interesting segementation happen in the heart of the cluster, likely along a the dimensions least captured by the principal components.
                </figcaption>
            </figure>
            <figure>
                <img class="figure" src="/csci5622/figures/clustering/silhouette.png">
                <figcaption>The silhouette graph for K. We can see that the minimum Silhouette Score is 0.21 when n=3.</figcaption>
            </figure>
            <figure>
                <img class="figure" src="/csci5622/figures/clustering/hier_cos.png">
                <figcaption>Here is the dendrogram for hierarchical clustering with the distance metric as cosine similarity.</figcaption>
            </figure>
            <figure>
                <img class="figure" src="/csci5622/figures/clustering/hier_euc.png">
                <figcaption>Here is the dendrogram for hierarchical clustering with the distance metric as euclidean distance.</figcaption>
            </figure>
            <figure>
                <img class="figure" src="/csci5622/figures/clustering/elbow.png">
                <figcaption>Finally, one last graph from R about the optimal number of clusters. Interstingly, this doesn't agree with Python.</figcaption>
            </figure>
            <h3>Conclusions</h3>
            To draw conclusions about the relationship between demographics and wealth index, it's necessary to first compute the mean values for each variable for each of the clusters to find their "centers". Next, these scaled values for cluster centers can be multiplied times the standard deviation for each column, and then added to the mean, so that they are effectively "unscaled". Next, the mean value of the label (in this case, wealth index) is computed for each of the clusters for each of set of k-means. This shows the average wealth of each cluster. These results are shown in the table below:
            <figure>
                <img class="figure" src="/csci5622/figures/clustering/clustercenters.png">
                <figcaption>The centers of each cluster as well as the mean values for wealth index.</figcaption>
            </figure>
            Finally, the results of the clustering are interpretable. Going through each value of k, the following can be said about each of the clusters:
            <ul>
                <li>2 Clusters: Here, there is a pretty clear split between large and small households. Cluster B averages over 2 women per household along with over 8 people total. These households are slightly wealthier than their smaller counterparts. 
                </li>
                <li>3 Clusters: This is the number of clusters for which the silhouette score is lowest. Here, the wealthiest group of households are also the largest, with 2.6 women and 10.3 total members. Their houses are accordingly larger, averaging 4.6 rooms. The average number of men is 1.4, suggesting these households are multiple families living in the same compound. Ironically, the poorest group is the middle one, that have about 4 children or dependents, but many of which have no men in the household. These fare worse than  the smallest households have have just 2 children or dependents. It seems being a single parent of multiple dependents is tough.
                </li>
                <li>4 Clusters:  Here there is a similar trend as with 3 clusters in that the smallest households are not the poorest, but instead the households with one woman taking care of many dependents (4.3) but few men (0.3) fare the worst. In contrast, households with  the most men (2.5) fare the best, even better than the larger households with more women, total members, and rooms for sleeping. This suggests that asset accrual is largely driven by men, not by women. This makes sense in a patriarchal society that expects women to take care of the family.  
                </li>
                <li>5 Clusters: This clustering largely shows the same patters found in the 4-cluster grouping. The poorest households are not the the smallest ones, but the ones with lots of babies but few men. 
                </li>
            </ul>
            It should be noted as well that these trends are slight (z-scores all smaller than 0.3). This is inline with the initial graph created with clusters that shows a high degree of dispersion of the wealth quintiles. Further resesearch should take into account non-numerical variables as well, like the algorithm k-prototype. Hopefully this would provide a better predictive model for wealth index than just using household demographics.
            <h3>References</h3>
            <p class="reference">Seif, George. 2018. Towards Data Science. https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know</p>
            <p class="reference">Wamala, Lydia. 2022. World Food Project. https://www.wfp.org/stories/rural-sierra-leone-fatmatas-kids-wont-be-eating</p>

        </div>
    </div>

</body>

</html>