<!DOCTYPE html>
<html>

<head>
    <link rel="stylesheet" href="/style.css">
    <script src="https://kit.fontawesome.com/f4ea09cda5.js" crossorigin="anonymous"></script>
</head>

<body>
    <button class="back-button" onclick="history.back()"><i class="fa-solid fa-arrow-left"></i></button>
    <div class="content">
        <div class="textbox">            
            <!-- ML Assignment
                (a) Overview: Here, describe clustering, including partitional vs, hierarchical. Note the distance metrics used. Discuss how you plan to use clustering to engage in discovery. Have two images. More is fine.
                (b) Data Prep. All models and methods require specific data formats. Clustering requires ONLY unlabeled numeric data. Explain this and show an image of the sample of data you plan to use. LINK to the sample of data as well. 
                (c) Code. Use Python to code k-means clustering and use R (hclust) to code hierarchical clustering. Use Cosine Similarity as the distance measure for the hclust. LINK to the code.
                (d) Results. Discuss, illustrate, describe, and visualize the results. Have at least one dendrogram and at least one clustering image (that is not a dendrogram). What values of k did you use for k - means (you should use at least 3 different k values to compare). What did the Silhouette method tell you about the "best k". Include a vis of the silhouette results. Using the hierarchical clustering, how did this compare and coincide with the k-means? What value of k does hclust suggest?
                (e) Conclusions. What did you learn that pertains to your topic? 
            -->
            <h1>Clustering</h1>

            <h3>Overview</h3>
            <p>
                Clustering is a set of unsupervised learning algorithms that serve to group like data together. In order to be able to group data by "similarity",
                all clustering methods must define a distance metric that gives the "distance" between any two points in the dataset, say A and B. The distance
                function can be any function that (a) is always >= 0, is 0 if A and B are the same point, and cannot be shortcutted through a third point C. That is,
                the distance D(A,B) must be >= D(A,C) + D(B,C). No wormholes! Common distance metrics include minkowski distances with p=1 (Manhattan Distance) and 
                p=2 (Euclidean Distance), as well as consine similarity. One you have a distance metric, there are three main approaches to clustering: 
                <ol>
                    <li><b>Partitional Clustering:</b> in this approach, "partitions" are created between groups that become clustered around centroids. Examples of this
                        are k-means, k-modes, and k-prototypes.
                    </li>
                    <li><b>Hierachical Clustering:</b> in this approach, elements are grouped together in a hierarchical tree, with similar elements belonging to the same
                    branches. Algorithms can start from the top-down and divide (such as DIANA), or start from the bottom-up and agglomerate (such as AGNES).</li>
                    <li><b>Density Clustering:</b> in this approach, elements are group with their closest neighbors iteratively. This way, clusters of points can be formed
                    that span overlapping values of the various dimensions, but are shaped in a relatively continuous blob.</li>
                </ol>
            <figure>
                <img class="figure" src="/csci5622/figures/clustering/clustering.png">
                <figcaption>Examples of various types of clustering algorithms (Seif, 2018).</figcaption>
            </figure>
            <p>
                In this project, clustering is used to determine how many meaningful groups there are among households in the survey based on the number of people 
                that comprise the household, the number of rooms of the house, and a wealth index score. The outcome of this clustering will tell how many socio-demographic
                strata there are within this dataset.
            </p>
            <figure>
                <img class="figure" src="/csci5622/figures/clustering/slhome.jpg">
                <figcaption>A single-mother household in rural Sierra Leone (Wamala, 2022).</figcaption>
            </figure>
            
            <h3>Data Prep</h3>
            <p>
                For this dataset, clustering is performed based on euclidean distance and based on cosine similarity. Both of these algorithms require continuous, numeric
                data for each of the dimensions, and so the data has to be cleaned. Of the 50 or so variables kept from the cleaning process, only 9 of them have clean
                numeric data. Of these variables, only 6 are determined to be of interest:
                <ol>
                    <li>hv010: Number of eligible women in household</li>
                    <li>hv011: Number of eligible men in household</li>
                    <li>hv012: Number of de jure members</li>
                    <li>hv014: Number of children 5 and under (de jure)</li>
                    <li>hv216: Number of rooms used for sleeping</li>
                    <li>hv270a: Wealth index for urban/rural</li>
                </ol>
                In this clustering analysis, demographics will be used to predict wealth index for urban rural, and so the final variable hv270a is removed as it is the 
                "label". Below, find snapshots of the data post-processing.
            </p>
            <figure>
                <img class="figure" src="/csci5622/figures/clustering/numdata.png">
                <figcaption>A snapshot of the dataset ready to be scaled for clustering. A copy of this numerical dataset can be found 
                    <a href="https://github.com/isaiahlg/csci5622mod2/blob/main/proj/data/sl19num.csv">here</a>.</figcaption>
            </figure>
            <figure>
                <img class="figure" src="/csci5622/figures/clustering/scaleddata.png">
                <figcaption>A snapshot of the dataset after normalization, ready for clustering. A copy of this scaled dataset can be found 
                    <a href="https://github.com/isaiahlg/csci5622mod2/blob/main/proj/data/sl19scaled.csv">here</a>.</figcaption>
            </figure>
            <p>
                In order to see if there are any patterns in the clustering, a principal component analysis is run of the remaining 5 numerical variables down to 2 
                principal components. This allows for a 2D plot of the data. The label is added back into the PCA data frame to be graphed with color, with the results
                below:
            </p>
            <figure>
                <img class="figure" src="/csci5622/figures/clustering/pcaclusters.png">
                <figcaption>A 2D plot of the principal components of the numerical data with the wealth index overlaid in color. Here one can see that 
                    the correlation between the demographic variables being used and wealth index is not very strong, since the colors are all a bit disperesed.
                </figcaption>
            </figure>
            <h3>Code</h3>
            <p>Python was used to perform k-means clustering and principal component analysis with the code <a href="https://github.com/isaiahlg/csci5622mod2/blob/main/proj/kmeans.py">here</a>. 
                R was used to perform hierarchical clustering with the code <a href="https://github.com/isaiahlg/csci5622mod2/blob/main/proj/hclust.Rmd">here</a>. 
            </p>
            
            <h3>Results</h3>
            <p>
                Clustering was run for k = [2,3,4,5]. Silhouette analysis was performed for each of these k-values, and the resulting charts are below:
            </p>
            <figure>
                <img class="figure" src="/csci5622/figures/clustering/kmeans2.png">
                <figcaption>Clustering results for k=2, graphed along the two principal components. Unsurprisingly, the split is almost entirely along
                    the first principal component.
                </figcaption>
            </figure>
            <figure>
                <img class="figure" src="/csci5622/figures/clustering/kmeans3.png">
                <figcaption>Clustering results for k=3, graphed along the two principal components. Here, the partitions are still primarily
                    along the first principal component.
                </figcaption>
            </figure>
            <figure>
                <img class="figure" src="/csci5622/figures/clustering/kmeans4.png">
                <figcaption>Clustering results for k=4, graphed along the two principal components. Here, we see a fourth category emerge 
                    on top defined primarily by high values of the second principal component.
                </figcaption>
            </figure>
            <figure>
                <img class="figure" src="/csci5622/figures/clustering/kmeans5.png">
                <figcaption>Clustering results for k=5, graphed along the two principal components. Here, we seen interesting segementation
                    happen in the heart of the cluster, likely along a the dimensions least captured by the principal components.
                </figcaption>
            </figure>
            <figure>
                <img class="figure" src="/csci5622/figures/clustering/silhouette.png">
                <figcaption>The silhouette graph for K. We can see that the minimum Silhouette Score is 0.212 when n=3.</figcaption>
            </figure>
            <h3>Conclusions</h3>
            <h3>References</h3>
            <p class="reference">Seif, George. 2018. Towards Data Science. https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68</p>
            <p class="reference">Wamala, Lydia. 2022. World Food Project. https://www.wfp.org/stories/rural-sierra-leone-fatmatas-kids-wont-be-eating-tonight</p>
            

        </div>
    </div>

</body>

</html>